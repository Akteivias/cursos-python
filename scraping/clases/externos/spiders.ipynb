{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inheriting the Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_class(c):\n",
    "  newc = c()\n",
    "  meths = dir(newc)\n",
    "  if 'name' in meths:\n",
    "    print(\"Your spider class name is:\", newc.name)\n",
    "  if 'from_crawler' in meths:\n",
    "    print(\"It seems you have inherited methods from scrapy.Spider -- NICE!\")\n",
    "  else:\n",
    "    print(\"Oh no! It doesn't seem that you are inheriting the methods from scrapy.Spider!!\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    pass\n",
    "  # parse method\n",
    "  def parse(self, response):\n",
    "    pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class(YourSpider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>When learning about <code>scrapy</code> spiders, we saw that the main portion of the code for us to adjust is the <code>class</code> for the spider. To help build some familiarity of the class, you will complete a short piece of code to complete a toy-model of the spider class code. We've omitted the code that would actually <em>run</em> the spider, only including the pieces necessary to create the class. </p>\n",
    "<p>As mentioned in the lesson, a <code>class</code> is roughly a collection of related variables and functions housed together. Sometimes one class likes to use methods from another class, and so we will <em>inherit</em> methods from a different class. That's what we do in the spider class.</p>\n",
    "<p>We wrote the function <code>inspect_class</code> to look at the your class once you're done, if you'd like to test your solution!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Pass <code>scrapy.Spider</code> as an argument to the class <code>YourSpider</code>; this will make it so that <code>YourSpider</code> <em>inherits</em> the methods from <code>scrapy.Spider</code>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>As stated in the instructions, all you need to do is pass <code>scrapy.Spider</code> as an argument into <code>YourSpider</code>!</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hurl the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_class( c ):\n",
    "  newc = c()\n",
    "  meths = dir( newc )\n",
    "  if 'start_requests' in meths:\n",
    "    print( \"The start_requests method yields the following urls:\" )\n",
    "    for u in newc.start_requests():\n",
    "      print(  \"\\t-\", u )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    urls = [\"https://www.datacamp.com\", \"https://scrapy.org\"]\n",
    "    for url in urls:\n",
    "      yield url\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the next lesson we will talk about the <code>start_requests</code> method within the spider class. In this quick exercise, we ask you to change around a variable within the <code>start_requests</code> method which foreshadows some of what we will be learning in the next lesson. Basically, we want you to start becoming comfortable turning some of the wheels within a spider class; in this case, making a list of <code>urls</code> within the <code>start_requests</code> method. </p>\n",
    "<p>We've written a function <code>inspect_class</code> which will print out the list of elements you have in the <code>urls</code> variable within the <code>start_requests</code> method.</p>\n",
    "<p><strong>Note</strong>: in the next several exercises, you will write code to complete your spider class, but the code does not yet include the pieces to actually <strong>run</strong> the spider; that will come at the end.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Fill in the blank within the <code>start_requests</code> method to assign the variable <code>urls</code> a list with the two strings: <code>\"https://www.datacamp.com\"</code> and<code>\"https://scrapy.org\"</code>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Set <code>urls</code> in <code>start_requests</code> equal to a list with the two elements being the two strings in the instructions!</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Referencing is Classy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_class( c ):\n",
    "  newc = c()\n",
    "  try:\n",
    "    newc.start_requests()\n",
    "  except:\n",
    "    print( \"Oh No! Something is wrong with the code! Keep trying.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    self.print_msg( \"Hello World!\" )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  # print_msg method\n",
    "  def print_msg( self, msg ):\n",
    "    print( \"Calling start_requests in YourSpider prints out:\", msg )\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>You probably have noticed that within the spider class, we always input the argument <code>self</code> in the <code>start_requests</code> and <code>parse</code> methods (just look in the sample code in this exercise!). This allows us to reference between methods within the class. That is, if we want to refer to the method <code>parse</code> within the <code>start_requests</code> method, we would need to write <code>self.parse</code> rather than just <code>parse</code>; what writing <code>self</code> does is tell the code: \"Look in the same class as <code>start_requests</code> for a method called <code>parse</code> to use.\" </p>\n",
    "<p>In this exercise you will get a chance to play with this \"self referencing\".</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Fill in the required <code>scrapy</code> object into the class <code>YourSpider</code> needed to create the <code>scrapy</code> spider.</li>\n",
    "<li>Pass the string argument <code>\"Hello World!\"</code> to fill in the blank in the <code>start_requests</code> method to use the <code>print_msg</code> method.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Remember that your scrapy spider needs to inherit <code>scrapy.Spider</code>.</li>\n",
    "<li>Within <code>start_requests</code>, all you need to do is pass the <code>\"Hello World\"</code> string as an argument to the call of <code>start.print_msg</code>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting with Start Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_class( c ):\n",
    "  newc = c()\n",
    "  try:\n",
    "    y = list( newc.start_requests() )\n",
    "    first_yield = y[0]\n",
    "    print( \"The url you would scrape is:\", first_yield.url )\n",
    "    cb = first_yield.callback\n",
    "    print( \"The name of the callback method you called is:\", cb.__name__ )\n",
    "  except:\n",
    "    print( \"Oh No! Something is wrong with the code! Keep trying.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url = \"https://www.datacamp.com\", callback = self.parse )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the last lesson we learned about setting up the <code>start_requests</code> method within a <code>scrapy</code> spider. Here we have another toy-model spider which doesn't actually scrape anything, but gives you a chance to play with the start_requests method. What we want is for you to start becomming familiar with the arguments you pass into the <code>scrapy.Request</code> call within <code>start_requests</code>. </p>\n",
    "<p>As before, we have created the function <code>inspect_class</code> to examine what you are yielding in <code>start_requests</code>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Fill in the required <code>scrapy</code> object into the class <code>YourSpider</code> needed to create the <code>scrapy</code> spider.</li>\n",
    "<li>Fill in the blank in the yielded <code>scrapy.Request</code> call within the <code>start_requests</code> method so that the URL this spider would start scraping is <code>\"https://www.datacamp.com\"</code> and would use the <code>parse</code> method (within the <code>YourSpider</code> class) as the method to parse the website.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Remember that you will use two arguments in <code>start_requests</code>, the first is <code>url</code> and the second is <code>callback</code>.</li>\n",
    "<li>Remember to use <code>self.parse</code> to reference the method <code>parse</code> within the <code>start_requests</code> method.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pen Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "\n",
    "def inspect_spider( s ):\n",
    "  news = s()\n",
    "  try:\n",
    "    req = list( news.start_requests() )[0]\n",
    "    url = req.url\n",
    "    html = requests.get( url ).content\n",
    "    response = TextResponse( url = url, body = html, encoding = 'utf-8' )\n",
    "    author_names = req.callback( response )\n",
    "    print( 'You have collected the author names:')\n",
    "    for a in author_names:\n",
    "      print('\\t-', a )\n",
    "  except:\n",
    "    print( 'Oh no! Something went wrong with the code. Keep trying!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the Spider class\n",
    "class DCspider( scrapy.Spider ):\n",
    "  name = 'dcspider'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    # Create an extracted list of course author names\n",
    "    author_names = response.css( 'p.course-block__author-name::text' ).extract()\n",
    "    # Here we will just return the list of Authors\n",
    "    return author_names\n",
    "  \n",
    "# Inspect the spider\n",
    "inspect_spider( DCspider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this exercise, we have set up a spider class which, when finished, will retrieve the author names from a shortened version of the DataCamp course directory. The URL for the shortened version is stored in the variable <code>url_short</code>. Your job will be to create the list of <strong>extracted</strong> author names in the <code>parse</code> method of the spider. </p>\n",
    "<p>Two things you should know:</p>\n",
    "<ul>\n",
    "<li>You will be using the <code>response</code> object and the <code>css</code> method here. </li>\n",
    "<li>The course author names are defined by the <strong>text</strong> within the paragraph <code>p</code> elements belonging to the class <code>course-block__author-name</code></li>\n",
    "</ul>\n",
    "<p>You can inspect the spider using the function <code>inspect_spider()</code> that we built for you -- it will print out the author names you find!</p>\n",
    "<p><strong>Note that this and the remaining exercises in this chapter may take some time to load.</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Fill in the required arguments to the parse method so that it will work as required when called in the <code>start_requests</code> method. </li>\n",
    "<li>Within the <code>parse</code> method, <strong>create</strong> a variable <code>author_names</code>, which is a list of strings created by extracting the text from the paragraph elements belonging to the class <code>course-block__author-name</code>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Remember that one of the two arguments <code>parse</code> needs to be passed is <code>self</code>.</li>\n",
    "<li>When using <code>response.css</code> to get to the author names, make sure to extract the text.</li>\n",
    "<li>Remember that in CSS Locator notation, use a period <code>.</code> when trying to select an element by class.</li>\n",
    "<li>Remember that in CSS Locator notation, to select text, you use <code>::text</code> appropriately in the string.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawler Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "\n",
    "def print_first_descr( course_descrs ):\n",
    "  print( \"The first course description is: \\n\", course_descrs[0] )\n",
    "  \n",
    "def inspect_spider( s ):\n",
    "  news = s()\n",
    "  try:\n",
    "    req1 = list( news.start_requests() )[0]\n",
    "    html1 = requests.get( req1.url ).content\n",
    "    response1 = TextResponse( url = req1.url, body = html1, encoding = 'utf-8' )\n",
    "    req2 = list( news.parse( response1 ) )[0]\n",
    "    html2 = requests.get( req2.url ).content\n",
    "    response2 = TextResponse( url = req2.url, body = html2, encoding = 'utf-8' )\n",
    "    for d in news.parse_descr( response2 ):\n",
    "      print(\"One course description you found is:\", d )\n",
    "      break\n",
    "  except:\n",
    "    print(\"Oh no! Something is wrong with the code. Keep trying!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the Spider class\n",
    "class DCdescr( scrapy.Spider ):\n",
    "  name = 'dcdescr'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  \n",
    "  # First parse method\n",
    "  def parse( self, response ):\n",
    "    links = response.css( 'div.course-block > a::attr(href)' ).extract()\n",
    "    # Follow each of the extracted links\n",
    "    for link in links:\n",
    "      yield response.follow( url = link, callback = self.parse_descr )\n",
    "      \n",
    "  # Second parsing method\n",
    "  def parse_descr( self, response ):\n",
    "    # Extract course description\n",
    "    course_descr = response.css( 'p.course__description::text' ).extract_first()\n",
    "    # For now, just yield the course description\n",
    "    yield course_descr\n",
    "\n",
    "\n",
    "# Inspect the spider\n",
    "inspect_spider( DCdescr )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This will be your first chance to play with a spider which will crawl between sites (by first collecting links from one site, and following those links to parse new sites). This spider starts at the shortened DataCamp course directory, then extracts the links of the courses in the <code>parse</code> method; from there, it will follow those links to extract the course descriptions from each course page in the <code>parse_descr</code> method, and put these descriptions into the list <code>course_descrs</code>. Your job is to complete the code so that the spider runs as desired!</p>\n",
    "<p>We have created a function <code>inspect_spider</code> which will print out one of the course descriptions you scrape (if done correctly)!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Fill in the two blanks below (one in each of the parsing methods) with the appropriate entries so that the spider can move from the first parsing method to the second correctly.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Remember that <code>response.follow</code> works in a similar way to <code>scrapy.Request</code>.</li>\n",
    "<li>Don't forget that the first of two entries for a parsing method is <code>self</code>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "\n",
    "def previewCourses( dc_dict, n = 3 ):\n",
    "  crs_titles = list( dc_dict.keys() )\n",
    "  print( \"A preview of DataCamp Courses:\")\n",
    "  print(\"---------------------------------------\\n\")\n",
    "  for t in crs_titles[:n]:\n",
    "    print( \"TITLE: %s\" % t)\n",
    "    for i,ct in enumerate(dc_dict[t]):\n",
    "      print(\"\\tChapter %d: %s\" % (i+1,ct) )\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Chapter_Spider(scrapy.Spider):\n",
    "  name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "  def parse_front(self, response):\n",
    "    course_blocks = response.css('div.course-block')\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    links_to_follow = course_links.extract()\n",
    "    for url in links_to_follow:\n",
    "      yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "  def parse_pages(self, response):\n",
    "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    ch_titles = response.css('h4.chapter__title::text')\n",
    "    ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "    dc_dict[ crs_title_ext ] = ch_titles_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Chapter_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the last lesson, we went through creating an entire web-crawler to access course information from each course in the DataCamp course directory. However, the lesson seemed to stop without a climax, because we didn't play with the code after finishing the parsing methods. </p>\n",
    "<p>The point of this exercise is to remedy that!</p>\n",
    "<p>The code we give you to look at in this and the next exercise is long, because its the entire spider that took us the lesson to create! However, don't be intimidated! The point of these two exercises is to give you a <strong>very</strong> easy task to complete, with the hope that you will look at and run the code for this spider. That way, even though it is long, you will have a grasp of it!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Fill in the one blank at the end of the <code>parse_pages</code> methods to assign the chapter titles to the dictionary whose key is the corresponding course title.</li>\n",
    "</ul>\n",
    "<p><strong>NOTE</strong>: If you hit Run Code, you must Reset to Sample Code to successfully use Run Code again!!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>The chapter titles are within the variable <code>ch_titles</code>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataCamp Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "\n",
    "def previewCourses( dc_dict, n = 1 ):\n",
    "  crs_titles = list( dc_dict.keys() )\n",
    "  print( \"A preview of DataCamp Courses:\")\n",
    "  print(\"---------------------------------------\\n\")\n",
    "  for t in crs_titles[:n]:\n",
    "    print( \"TITLE: %s\" % t)\n",
    "    print(\"\\tDescription: %s\" % dc_dict[t] )\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Description_Spider(scrapy.Spider):\n",
    "  name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "  def parse_front(self, response):\n",
    "    course_blocks = response.css('div.course-block')\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    links_to_follow = course_links.extract()\n",
    "    for url in links_to_follow:\n",
    "      yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "  def parse_pages(self, response):\n",
    "    # Create a SelectorList of the course titles text\n",
    "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "    # Extract the text and strip it clean\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    # Create a SelectorList of course descriptions text\n",
    "    crs_descr = response.css('p.course__description::text')\n",
    "    # Extract the text and strip it clean\n",
    "    crs_descr_ext = crs_descr.extract_first().strip()\n",
    "    # Fill in the dictionary\n",
    "    dc_dict[crs_title_ext] = crs_descr_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Description_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Like the previous exercise, the code here is long since you are working with an entire web-crawling spider! But again, don't let the amount of code intimidate you, you have a handle on how spiders work now, and you are perfectly capable to complete the easy task for you here!</p>\n",
    "<p>As in the previous exercise, we have created a function <code>previewCourses</code> which lets you preview the output of the spider, but you can always just explore the dictionary <code>dc_dict</code> too after you run the code.</p>\n",
    "<p>In this exercise, you are asked to create a CSS Locator string direct to the text of the course description. All you need to know is that from the course page, the course description text is within a paragraph <code>p</code> element which belongs to the class <code>course__description</code> (two underlines).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Fill in the one blank below in the <code>parse_pages</code> method with a CSS Locator string which directs to the text within the paragraph <code>p</code> element which belongs to the class <code>course__description</code>.</li>\n",
    "</ul>\n",
    "<p><strong>NOTE</strong>: If you hit Run Code, you must Reset to Sample Code to successfully use Run Code again!!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Remember that using a period <code>.</code> in CSS Locator notation tells us to select by class.</li>\n",
    "<li>Remember that to select text in CSS Locator notation, you need to use <code>::text</code> somewhere.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.http import TextResponse\n",
    "import requests\n",
    "\n",
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "\n",
    "html = requests.get( url_short ).content\n",
    "\n",
    "response = TextResponse( url = url_short, body = html, encoding = 'utf-8' )\n",
    "\n",
    "dc_dict = dict()\n",
    "\n",
    "def previewCourses( dc_dict = dc_dict, n = 3 ):\n",
    "  parse( self = None, response = response )\n",
    "  crs_titles = list( dc_dict.keys() )\n",
    "  print( \"A preview of DataCamp Courses:\")\n",
    "  print(\"---------------------------------------\\n\")\n",
    "  for t in crs_titles[:n]:\n",
    "    print( \"TITLE: %s\" % t)\n",
    "    print( \"\\tDESCRIPTION: %s\" % dc_dict[t] )\n",
    "    print(\"\")\n",
    "    \n",
    "# Does this fix the testing issue?\n",
    "self = None\n",
    "\n",
    "def parse( self, response ):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This exercise gives you a chance to show off what you've learned! In this exercise, you will write the parse function for a spider and then fill in a few blanks to finish off the spider. On the course directory page of DataCamp, each listed course has a title and a short course description. This spider will be used to scrape the course directory to extract the course titles and short course descriptions. You will not need to follow any links this time. Everything you need to know is:</p>\n",
    "<ul>\n",
    "<li>The course titles are defined by the text within an <code>h4</code> element whose class contains the string <code>block__title</code> (double underline).</li>\n",
    "<li>The short course descriptions are defined by the text within a paragraph <code>p</code> element whose class contains the string <code>block__description</code> (double underline).</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
