{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "my_string = \"Let's write RegEx!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Which of the following Regex patterns results in the following text? </p>\n",
    "<pre><code class=\"{python} language-{python}\">&gt;&gt;&gt; my_string = \"Let's write RegEx!\"\n",
    "&gt;&gt;&gt; re.findall(PATTERN, my_string)\n",
    "['Let', 's', 'write', 'RegEx']\n",
    "</code></pre>\n",
    "<p>In the IPython Shell, try replacing <code>PATTERN</code> with one of the below options and observe the resulting output. The <code>re</code> module has been pre-imported for you and <code>my_string</code> is available in your namespace.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Which pattern will match upper and lowercase characters? Remember, the + sign will make the pattern greedy!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practicing regular expressions: re.split() and re.findall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now you'll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. Take a look at <code>my_string</code> first by printing it in the IPython Shell, to determine how you might best match the different steps.</p>\n",
    "<p>Note: It's important to prefix your regex patterns with <code>r</code> to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. For example, <code>\"\\n\"</code> in Python is used to indicate a new line, but if you use the <code>r</code> prefix, it will be interpreted as the raw string <code>\"\\n\"</code> - that is, the character <code>\"\\\"</code> followed by the character <code>\"n\"</code> - and not as a new line.</p>\n",
    "<p>The regular expression module <code>re</code> has already been imported for you.</p>\n",
    "<p><em>Remember from the video that the syntax for the regex library is to always to pass the <strong>pattern first</strong>, and then the <strong>string second</strong>.</em></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Split <code>my_string</code> on each sentence ending. To do this:<ul>\n",
    "<li>Write a pattern called <code>sentence_endings</code> to match sentence endings (<code>.?!</code>).</li>\n",
    "<li>Use <code>re.split()</code> to split <code>my_string</code> on the pattern and print the result.</li></ul></li>\n",
    "<li>Find and print all capitalized words in <code>my_string</code> by writing a pattern called <code>capitalized_words</code> and using <code>re.findall()</code>. <ul>\n",
    "<li>Remember the <code>[a-z]</code> pattern shown in the video to match lowercase groups? Modify that pattern appropriately in order to match uppercase groups.</li></ul></li>\n",
    "<li>Write a pattern called <code>spaces</code> to match one or more spaces (<code>\"\\s+\"</code>) and then use <code>re.split()</code> to split <code>my_string</code> on this pattern, keeping all punctuation intact. Print the result.</li>\n",
    "<li>Find all digits in <code>my_string</code> by writing a pattern called <code>digits</code> (<code>\"\\d+\"</code>) and using <code>re.findall()</code>. Print the result.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Remember, you can use <code>\"\\w\"</code> to match alphanumeric, <code>\"\\d\"</code> to match digits, <code>\"\\s\"</code> to match spaces and <code>\"+\"</code> to make anything greedy. </li>\n",
    "<li>For groupings, you can use square brackets <code>[]</code> to declare part of the pattern or the entire pattern.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, urlretrieve\n",
    "import os\n",
    "\n",
    "os.makedirs('tokenizers/punkt/PY3/')\n",
    "urlretrieve('https://s3.amazonaws.com/assets.datacamp.com/production/course_3747/datasets/english_pickle.txt', 'tokenizers/punkt/PY3/english.pickle')\n",
    "holy_grail = urlopen('https://s3.amazonaws.com/assets.datacamp.com/production/course_3747/datasets/grail.txt').read().decode('utf-8')\n",
    "\n",
    "scene_one = holy_grail[:holy_grail.find(\"SCENE 2\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here, you'll be using the first scene of Monty Python's Holy Grail, which has been pre-loaded as <code>scene_one</code>. Feel free to check it out in the IPython Shell!</p>\n",
    "<p>Your job in this exercise is to utilize <code>word_tokenize</code> and <code>sent_tokenize</code> from <code>nltk.tokenize</code> to tokenize both words and sentences from Python strings - in this case, the first scene of Monty Python's Holy Grail.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Import the <code>sent_tokenize</code> and <code>word_tokenize</code> functions from <code>nltk.tokenize</code>.</li>\n",
    "<li>Tokenize all the sentences in <code>scene_one</code> using the <code>sent_tokenize()</code> function.</li>\n",
    "<li>Tokenize the fourth sentence in <code>sentences</code>, which you can access as <code>sentences[3]</code>, using the <code>word_tokenize()</code> function. </li>\n",
    "<li>Find the unique tokens in the entire scene by using <code>word_tokenize()</code> on <code>scene_one</code> and then converting it into a set using <code>set()</code>.</li>\n",
    "<li>Print the unique tokens found. This has been done for you, so hit 'Submit Answer' to see the results!</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Use the command <code>from y import x</code> to import <code>x</code> from <code>y</code>.</li>\n",
    "<li>Use the <code>sent_tokenize()</code> function to tokenize the sentences in <code>scene_one</code>.</li>\n",
    "<li>Use <code>word_tokenize()</code> to tokenize the appropriate sentence in <code>sentences</code>. Remember, Python uses 0-based numbering.</li>\n",
    "<li>After using <code>word_tokenize()</code> on <code>scene_one</code>, use the <code>set()</code> function to convert it into a set.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More regex with re.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from urllib.request import urlopen, urlretrieve\n",
    "import os\n",
    "\n",
    "os.makedirs('tokenizers/punkt/PY3/')\n",
    "urlretrieve('https://s3.amazonaws.com/assets.datacamp.com/production/course_3747/datasets/english_pickle.txt', 'tokenizers/punkt/PY3/english.pickle')\n",
    "\n",
    "\n",
    "holy_grail = urlopen('https://s3.amazonaws.com/assets.datacamp.com/production/course_3747/datasets/grail.txt').read().decode('utf-8')\n",
    "scene_one = holy_grail[:holy_grail.find(\"SCENE 2\")]\n",
    "sentences = sent_tokenize(scene_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this exercise, you'll utilize <code>re.search()</code> and <code>re.match()</code> to find specific tokens. Both <code>search</code> and <code>match</code> expect regex patterns, similar to those you defined in an earlier exercise. You'll apply these regex library methods to the same Monty Python text from the <code>nltk</code> corpora.</p>\n",
    "<p>You have both <code>scene_one</code> and <code>sentences</code> available from the last exercise; now you can use them with <code>re.search()</code> and <code>re.match()</code> to extract and match more text.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "\n",
    "pattern1 = r\"(\\w+|\\?|!)\"\n",
    "\n",
    "pattern2 = r\"(\\w+|#\\d|\\?|!)\"\n",
    "\n",
    "pattern3 = r\"(#\\d\\w+\\?!)\"\n",
    "\n",
    "pattern4 = r\"\\s+\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Given the following string, which of the below patterns is the best tokenizer? If possible, you want to retain sentence punctuation as separate tokens, but have <code>'#1'</code> remain a single token.</p>\n",
    "<pre><code class=\"{python} language-{python}\">my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "</code></pre>\n",
    "<p>The string is available in your workspace as <code>my_string</code>, and the patterns have been pre-loaded as <code>pattern1</code>, <code>pattern2</code>, <code>pattern3</code>, and <code>pattern4</code>, respectively. </p>\n",
    "<p>Additionally, <code>regexp_tokenize</code> has been imported from <code>nltk.tokenize</code>. You can use <code>regexp_tokenize(string, pattern)</code> with <code>my_string</code> and one of the patterns as arguments to experiment for yourself and see which is the best tokenizer.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The <code>|</code> character operates like an <code>or</code> statement. Try using <code>regexp_tokenize()</code> with <code>my_string</code> and one of the patterns to see how each pattern tokenizes the string.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex with NLTK tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\"This is the best #nlp exercise ive found online! #python\", \"#NLP is super fun! <3 #learning\", \"Thanks @datacamp :) #nlp #python\"]\n",
    "#pattern2 = r\"([#|@]\\w+)\" # or r\"@\\w+|#\\w+\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Twitter is a frequently used source for NLP text and tasks. In this exercise, you'll build a more complex tokenizer for tweets with hashtags and mentions using <code>nltk</code> and regex. The <code>nltk.tokenize.TweetTokenizer</code> class gives you some extra methods and attributes for parsing tweets. </p>\n",
    "<p>Here, you're given some example tweets to parse using both <code>TweetTokenizer</code> and <code>regexp_tokenize</code> from the <code>nltk.tokenize</code> module. These example tweets have been pre-loaded into the variable <code>tweets</code>. Feel free to explore it in the IPython Shell!</p>\n",
    "<p><em>Unlike the syntax for the regex library, with <code>nltk_tokenize()</code> you pass the pattern as the <strong>second</strong> argument.</em></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-ascii tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, urlretrieve\n",
    "import os\n",
    "\n",
    "os.makedirs('tokenizers/punkt/PY3/')\n",
    "urlretrieve('https://s3.amazonaws.com/assets.datacamp.com/production/course_3747/datasets/english_pickle.txt', 'tokenizers/punkt/PY3/english.pickle')\n",
    "\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize\n",
    "german_text = \"Wann gehen wir Pizza essen? \\U0001F355 Und fährst du mit Über? \\U0001F695\"\n",
    "\n",
    "print(german_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÜ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using German with emoji!</p>\n",
    "<p>Here, you have access to a string called <code>german_text</code>, which has been printed for you in the Shell. Notice the emoji and the German characters!</p>\n",
    "<p>The following modules have been pre-imported from <code>nltk.tokenize</code>: <code>regexp_tokenize</code> and <code>word_tokenize</code>. </p>\n",
    "<p>Unicode ranges for emoji are:</p>\n",
    "<p><code>('\\U0001F300'-'\\U0001F5FF')</code>, <code>('\\U0001F600-\\U0001F64F')</code>, <code>('\\U0001F680-\\U0001F6FF')</code>, and <code>('\\u2600'-\\u26FF-\\u2700-\\u27BF')</code>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Tokenize all the words in <code>german_text</code> using <code>word_tokenize()</code>, and print the result.</li>\n",
    "<li>Tokenize only the capital words in <code>german_text</code>. <ul>\n",
    "<li>First, write a pattern called <code>capital_words</code> to match only capital words. Make sure to check for the German <code>Ü</code>! To use this character in the exercise, copy and paste it from these instructions.</li>\n",
    "<li>Then, tokenize it using <code>regexp_tokenize()</code>. </li></ul></li>\n",
    "<li>Tokenize only the emoji in <code>german_text</code>. The pattern using the unicode ranges for emoji given in the assignment text has been written for you. Your job is to use <code>regexp_tokenize()</code> to tokenize the emoji.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>To tokenize all the words in <code>german_text</code>, pass it in as an argument to <code>word_tokenize()</code>.</li>\n",
    "<li>The pattern to match only capital words is <code>r\"[A-ZÜ]\\w+\"</code>.</li>\n",
    "<li>To write the pattern to match emoji, separate the unicode ranges for emoji shown in the assignment text using <code>|</code>. After writing the patterns, be sure to tokenize using <code>regexp_tokenize()</code> and then print the results.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charting practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "\n",
    "holy_grail = urlopen('https://s3.amazonaws.com/assets.datacamp.com/production/course_3747/datasets/grail.txt').read().decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the script into lines: lines\n",
    "lines = holy_grail.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Try using your new skills to find and chart the number of words per line in the script using <code>matplotlib</code>. The Holy Grail script is loaded for you, and you need to use regex to find the words per line. </p>\n",
    "<p>Using list comprehensions here will speed up your computations. For example: <code>my_lines = [tokenize(l) for l in lines]</code> will call a function <code>tokenize</code> on each line in the list <code>lines</code>. The new transformed list will be saved in the <code>my_lines</code> variable.</p>\n",
    "<p>You have access to the entire script in the variable <code>holy_grail</code>. Go for it!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Split the script <code>holy_grail</code> into lines using the newline (<code>'\\n'</code>) character.</li>\n",
    "<li>Use <code>re.sub()</code> inside a list comprehension to replace the prompts such as <code>ARTHUR:</code> and <code>SOLDIER #1</code>. The pattern has been written for you. </li>\n",
    "<li>Use a list comprehension to tokenize <code>lines</code> with <code>regexp_tokenize()</code>, keeping <strong>only words</strong>. Recall that the pattern for words is <code>\"\\w+\"</code>.</li>\n",
    "<li>Use a list comprehension to create a list of line lengths called <code>line_num_words</code>.<ul>\n",
    "<li>Use <code>t_line</code> as your iterator variable to iterate over <code>tokenized_lines</code>, and then <code>len()</code> function to compute line lengths.</li></ul></li>\n",
    "<li>Plot a histogram of <code>line_num_words</code> using <code>plt.hist()</code>. Don't forgot to use <code>plt.show()</code> as well to display the plot.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Use the <code>.split()</code> method on <code>holy_grail</code> with the newline character (<code>'\\n'</code>) as the argument.</li>\n",
    "<li>Recall that <code>re.sub()</code> requires 3 arguments: The pattern, the replacement, and the string. The pattern is given for you; the replacement is <code>''</code> and the string is <code>l</code>.</li>\n",
    "<li>Use <code>regexp_tokenize()</code> as the output expression of your list comprehension, with <code>s</code> and <code>\"\\w+\"</code> as the arguments.</li>\n",
    "<li>To create <code>line_num_words</code>, use <code>len(t_line)</code> as the output expression of the list comprehension.</li>\n",
    "<li>Use <code>plt.hist()</code> with <code>line_num_words</code> as the argument to create the histogram, and then <code>plt.show()</code> to display it.</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
