import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time
from itertools import cycle
import warnings
warnings.filterwarnings('ignore')


#definimos la semilla
semilla = 42

#Obtenemos el nombre de las variables
### Cambiar nombre

diccionario = pd.read_excel('https://datasets-humai.s3.amazonaws.com/datasets/diccionario_terminos.xlsx')
diccionario

#Lectura del dataset

nba = pd.read_csv('https://datasets-humai.s3.amazonaws.com/datasets/nba_salarios_2021.csv')


# Limpiamos algunas de las variables
nba['position'] = nba['position'].str.replace('-.*','')
nba = nba.fillna(0)


# Observamos la informacion del dataset
nba.info()

# Graficamos la relacion entre las posicion y el salario del jugador 

fig, ax = plt.subplots(figsize=(10,7))
ax = sns.boxplot(x="position", y="salary", data=nba)
ax.set_title('Boxplots: salarios y posición de juego', fontweight='bold', fontsize=20, y=1.1)
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
ax.grid(axis='y', linestyle='--')
ax.set_ylabel('Salario', fontsize=10)
ax.set_xlabel('Posición', fontsize=10)
plt.show()

# Obtenemos las variables numericas y generamos la matriz de correlacion
nba_numericas = nba.select_dtypes(include=['float64','int64'])
matriz_correlacion = nba_numericas.corr()

# Graficamos la correlacion entre las variables

mask = np.zeros_like(matriz_correlacion, dtype=np.bool)
mask[np.triu_indices_from(mask)]= True

f, ax = plt.subplots(figsize=(17, 17))

heatmap = sns.heatmap(matriz_correlacion,
                      mask = mask,
                      square = True,
                      linewidths = .5,
                      cmap = 'coolwarm',
                      cbar_kws = {'shrink': .4,
                                'ticks': [-1, -.5, 0, 0.5, 1]},
                      vmin = -1,
                      vmax = 1,
                      annot = True, annot_kws={"fontsize":8})


ax.set_yticklabels(matriz_correlacion.columns, rotation = 0)
ax.set_xticklabels(matriz_correlacion.columns)

sns.set_style({'xtick.bottom': True}, {'ytick.left': True})

#Seleccionamos algunas variables y graficamos su correlacion 

nba_numericas_seleccion = nba_numericas[['salary','age','pts_per_g','gs','ast_per_g','blk_per_g']]

def corr(x, y, **kwargs):
    
    # Calculamos el valor de la correlacion
    coef = np.corrcoef(x, y)[0][1]
    # Generamos la etiqueta
    label = r'$\rho$ = ' + str(round(coef, 2))
    
    # Agregamos la etiqueta al grafico
    ax = plt.gca()
    ax.annotate(label, xy = (0.2, 0.95), size = 20, xycoords = ax.transAxes)
    

# Mapeamos los graficos en las ubicaciones correspondientes
g = sns.PairGrid(nba_numericas_seleccion)
g.map_diag(sns.histplot)
g.map_lower(sns.regplot)
g.map_upper(corr)

# Eliminamos las variables del nombre del jugador y el equipo
nba_lineal = nba.drop(['player','team_id'], axis = 1)

# Seleccionamos las variables numericas
nba_lineal_numericas = nba_lineal.select_dtypes(include=['float64','int64'])

# Seleccionamos las variables categoricas y las convertimos en dummies 
nba_lineal_cat = nba_lineal.select_dtypes(include=['object'])


nba_lineal_cat['position'].unique()

from sklearn.preprocessing import OneHotEncoder

# Definimos una instancia del transformer
one_hot_encoder = OneHotEncoder(categories=[['PG', 'SG', 'PF', 'SF', 'C']], drop='first')

# Realizamos el fit con los datos de entrenamiento
one_hot_encoder.fit(nba_lineal_cat[['position']])

# Accedemos a las categorias del encoder
one_hot_encoder.categories_

 # Generamos las variables dummies de la variable property type (notemos que tenemos 2 columnas!)
 matriz_dummies = one_hot_encoder.transform(nba_lineal_cat[['position']]).toarray()
 matriz_dummies

# Generamos los nombres de las variables dummies (notemos que tenemos 2 columnas!)
nombres_dummies = one_hot_encoder.get_feature_names(['position'])
nombres_dummies

# Generamos el dataframe con las variables dummies con las matrices y columnas
df_dummies = pd.DataFrame(matriz_dummies, columns=nombres_dummies, index=nba_lineal_cat.index)
df_dummies.head()

# Separamos las variables predictoras de la que va a ser preditcha 'salary'
nba_lineal = pd.concat([nba_lineal_numericas, df_dummies],axis=1)
X = nba_lineal.drop(['salary'], axis = 1)
reg_features = X.columns
y = nba_lineal['salary']


# Import el paquete sm de la libreria stats
import statsmodels.api as sm
from statsmodels.api import add_constant

# Construimos el modelo agregando una constante
X = sm.add_constant(X)
model = sm.OLS(y,X)
results = model.fit()

# Error Cuadratico Medio de los Residuos
print(f"ECM: {results.mse_resid}")

# Visualizamos el ajuste final
print(results.summary())

def coef_summary(results):
    '''
    Toma los resultado del modelo de OLS 
    
    Elimina el intercepto.
    '''
    # Creo un dataframe de los resultados del summary 
    coef_df = pd.DataFrame(results.summary().tables[1].data)
    
    # Agrego el nombre de las columnas
    coef_df.columns = coef_df.iloc[0]

    # Elimino la fila extra del intercepto
    coef_df=coef_df.drop(0)

    # Seteo el nombre de las variables como index
    coef_df = coef_df.set_index(coef_df.columns[0])

    # Convertimos a float los object 
    coef_df = coef_df.astype(float)

    # Obtenemos el error; (coef - limite inferior del IC)
    errors = coef_df['coef'] - coef_df['[0.025']
    
    # Agregamos los errores al dataframe
    coef_df['errors'] = errors

    # Eliminamos la variable const
    coef_df = coef_df.drop(['const'])

    # Ordenamos los coeficientes 
    coef_df = coef_df.sort_values(by=['coef'])

    ### Graficamos ###

    # x-labels
    variables = list(coef_df.index.values)
    
    # Agregamos la columna con el nombre de las variables
    coef_df['variables'] = variables
   
    return  coef_df

# Aplicamos la funcion coef_summary a los results

coef_df = coef_summary(results)

# Graficamos los p-valor
fig, ax = plt.subplots(figsize=(10,7))
coef_df = coef_df.sort_values(by='P>|t|', ascending = False)
ax = sns.barplot(x='variables', y='P>|t|', data=coef_df,
                 palette="Spectral")
ax.set_title('P-valor de los regresores', fontweight='bold', fontsize=20, y=1.1)
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
ax.grid(axis='y', linestyle='--')
ax.set_ylabel('P-valor', fontsize=10)
ax.set_xlabel('variables', fontsize=10)
plt.xticks(rotation=90, fontsize = 10 )
plt.yticks( fontsize = 10 )
plt.axhline(y = 0.05, color = 'black', linestyle = '--')
plt.show()

#Importamos StandardScaler de la libreria sklear
from sklearn.preprocessing import StandardScaler

# Instanciamos la funcion para escalar
scaler = StandardScaler()

# Seleccionamos las variables numericas
nba_lineal = nba.drop(['player','team_id'], axis = 1)
nba_lineal_numericas = nba_lineal.select_dtypes(include=['float64','int64'])
# Eliminamos la variable salary del escalado
nba_lineal_numericas= nba_lineal_numericas.drop(['salary'], axis = 1)
nba_lineal_numericas_columns = nba_lineal_numericas.columns

#Escalamos las variables
scaled = scaler.fit_transform(nba_lineal_numericas)

# Generamos un dataframe con las variables escaladas
nba_lineal_numericas = pd.DataFrame(scaled, columns = nba_lineal_numericas_columns)

# Generamos dummies con las variables categoricas 
nba_lineal_cat = nba_lineal.select_dtypes(include=['object'])
nba_lineal_cat = pd.get_dummies(nba_lineal_cat)


# Generamos el conjunto de datos para ajustar el modelo
nba_lineal = pd.concat([nba_lineal_numericas, df_dummies],axis=1)                   
X = nba_lineal.copy()
y = nba['salary']

# Construimos el modelo agregando una constante
X = sm.add_constant(X)
model_scaled = sm.OLS(y,X)
results_scaled = model_scaled.fit()

# Error Cuadratico Medio de los Residuos
print(f"ECM: {results_scaled.mse_resid}")

# Visualizamos el ajuste final
print(results_scaled.summary())

#Graficamos los coeficientes al 95% de confianza
coef_df_scaled = coef_summary(results_scaled)

# Graficamos los p-valor
fig, ax = plt.subplots(figsize=(10,7))
coef_df_scaled = coef_df_scaled.sort_values(by='P>|t|', ascending = False)
ax = sns.barplot(x='variables', y='P>|t|', data=coef_df_scaled,
                 palette="Spectral")
ax.set_title('P-valor de los regresores', fontweight='bold', fontsize=20, y=1.1)
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
ax.grid(axis='y', linestyle='--')
ax.set_ylabel('P-valor', fontsize=10)
ax.set_xlabel('variables', fontsize=10)
plt.xticks(rotation=90, fontsize = 10 )
plt.yticks( fontsize = 10 )
plt.axhline(y = 0.05, color = 'black', linestyle = '--')
plt.show()

print(f"MSE Scaled: {results_scaled.rsquared}",'vs.',f"MSE: {results.rsquared}" )
print(f"Adj MSE Scaled: {results_scaled.rsquared_adj}",'vs.',f" Adj MSE: {results.rsquared_adj}" )
print(f"p-value Scaled: {results_scaled.f_pvalue}",'vs.',f" p-value: {results.f_pvalue}" )

# Importamos la funcion trai_test_split
from sklearn.model_selection import train_test_split

# Preparamos el conjunto de datos

nba_reg = nba.drop(['player','team_id'], axis = 1)
nba_reg_numericas = nba_reg.select_dtypes(include=['float64','int64'])
nba_reg_numericas_columns = nba_reg_numericas.columns

nba_reg_object = df_dummies.copy()
nba_reg = pd.concat([nba_reg_numericas, nba_reg_object],axis=1)
                       
X = nba_reg.drop(['salary'], axis = 1)
y = nba_reg['salary']

# Separamos el conjunto de datos en train y test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=semilla)

# Importamos la funcion Lasso
from sklearn.linear_model import Lasso

# generamos 50 lambdas para evaluar los distintos escenarios
n_alphas = 50
alphas = np.logspace(-5, 7.2, n_alphas)

# Ajustamos  la regresion Lasso para los disntos valores de lambda que establecimos
coefs = []
for a in alphas:
    lasso = Lasso(alpha=a, fit_intercept=False)
    lasso.fit(X_train, y_train)
    coefs.append(lasso.coef_)

#Graficamos como varian las variables cuando se aumenta el lambda

fig, ax = plt.subplots(figsize=(10,7))

l1 = plt.plot(alphas, coefs,linewidth=2 )

ax.set_title('Regularización Lasso', fontweight='bold', fontsize=20)
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)

ax.set_ylabel('coeficientes', fontsize=10)
ax.set_xlabel('Log(lambda)', fontsize=10)

plt.xticks(fontsize = 10 )
plt.yticks( fontsize = 10 )

plt.show()

# Nos quedamos con las variables que sobreviven al proceso de selección
coeficientes = pd.DataFrame(coefs, columns =X_train.columns)

variables_importantes = coeficientes.loc[:,X_train.columns[coeficientes.loc[45:].any().values]]

# Graficamos nuevamente
fig, ax = plt.subplots(figsize=(10,7))

l1 = plt.plot(alphas, variables_importantes,linewidth=2 )

ax.set_title('Regularización Lasso', fontweight='bold', fontsize=20)
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
ax.legend(coeficientes.columns, fontsize=10)
ax.set_ylabel('coeficientes', fontsize=10)
ax.set_xlabel('Log(lambda)', fontsize=10)

plt.xticks(fontsize = 10 )
plt.yticks( fontsize = 10 )

plt.show()

# Instanciamos la funcion para escalar
scaler = StandardScaler()

# Seleccionamos las variables numericas que vamos a escalar
nba_reg = nba.drop(['player','team_id'], axis = 1)
nba_reg_numericas = nba_reg.select_dtypes(include=['float64','int64'])
nba_reg_numericas_columns = nba_reg_numericas.columns

# Escalamos las variables
scaled = scaler.fit_transform(nba_reg_numericas)
nba_reg_numericas = pd.DataFrame(scaled, columns = nba_reg_numericas_columns)

# Conformamos el nuevo conjunto de entrenamiento
nba_reg = pd.concat([nba_reg_numericas, df_dummies],axis=1)
                       
X = nba_reg.drop(['salary'], axis = 1)
y = nba_reg['salary']


#Separamos en train y test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=semilla)

# Ajsutamos nuevamente las distintas regresiones Lasso para los alphas planteados
n_alphas = 50
alphas = np.logspace(-5, 0, n_alphas)

coefs = []
for a in alphas:
    lasso = Lasso(alpha=a, fit_intercept=False)
    lasso.fit(X_train, y_train)
    coefs.append(lasso.coef_)

# Graficamos los resultados

fig, ax = plt.subplots(figsize=(10,7))

l1 = plt.plot(alphas, coefs,linewidth=2 )

ax.set_title('Regularización Lasso', fontweight='bold', fontsize=20)
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)

ax.set_ylabel('coeficientes', fontsize=10)
ax.set_xlabel('Log(lambda)', fontsize=10)

plt.xticks(fontsize = 10 )
plt.yticks( fontsize = 10 )

plt.show()

#Seleccionamos las variables que sobreviven mayor tiempo al proceso de seleccion
coeficientes = pd.DataFrame(coefs, columns =X_train.columns)

variables_importantes = coeficientes.loc[:,X_train.columns[coeficientes.loc[40:].any().values]]


# Graficamos
fig, ax = plt.subplots(figsize=(10,7))

l1 = plt.plot(alphas, variables_importantes,linewidth=2 )

ax.set_title('Regularización Lasso', fontweight='bold', fontsize=20)
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
ax.legend(coeficientes.columns, fontsize=10)
ax.set_ylabel('coeficientes', fontsize=10)
ax.set_xlabel('Log(lambda)', fontsize=10)

plt.xticks(fontsize = 10 )
plt.yticks( fontsize = 10 )

plt.show()

# Importamos Lasso CV
from sklearn.linear_model import LassoCV

# Ajustamos el modelo mediante Cross-Validation
t1 = time.time()
lasso = LassoCV(cv=20).fit(X_train, y_train)
t_lasso_cv = time.time() - t1

# Graficamos

EPSILON = 1e-4

fig, ax =plt.subplots(figsize=(10,7))
plt.semilogx(lasso.alphas_ + EPSILON, lasso.mse_path_, ':')
l1 =plt.plot(lasso.alphas_ + EPSILON, lasso.mse_path_.mean(axis=-1), 'k',
         label='Promedio entre folds', linewidth=2)
ax.axvline(lasso.alpha_ + EPSILON, linestyle='--', color='k',
            label='lambda: CV estimado')

ax.legend(fontsize=10)

ax.set_xlabel(r'$\lambda$', fontsize=10)
ax.set_ylabel('Error Cuadratico Medio', fontsize=10)
ax.set_title('Error cuadratico medio en cada fold: coordinate descent '
          '(train time: %.2fs)' % t_lasso_cv, fontweight='bold', fontsize=20)
plt.xticks(fontsize = 10 )
plt.yticks( fontsize = 10 )

ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)

plt.show()

print(f"El lambda final obtenido mediante CV es: {lasso.alpha_}" )

coeficientes_finales = pd.DataFrame([np.array(X_train.columns.tolist()),lasso.coef_]).T
coeficientes_finales.columns = ['feature','coeficiente']
coeficientes_finales

print(f"El modelo final cuenta con : {coeficientes_finales[coeficientes_finales['coeficiente']!=0].shape[0]}",' features' )

#Importamos el modelo
from sklearn.linear_model import Ridge

# Ajustamos el modelo para distintos valores de lambda
n_alphas = 30
alphas = np.logspace(-5,0, n_alphas)

coefs = []
for a in alphas:
    ridge = Ridge(alpha=a, fit_intercept=False)
    ridge.fit(X_train, y_train)
    coefs.append(ridge.coef_)

# Graficamos

fig, ax = plt.subplots(figsize=(10,7))

l1 = plt.plot(alphas, coefs,linewidth=2 )

ax.set_title('Regularización Ridge', fontweight='bold', fontsize=20)
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)

ax.set_ylabel('coeficientes', fontsize=10)
ax.set_xlabel('Log(lambda)', fontsize=10)

plt.xticks(fontsize = 10 )
plt.yticks( fontsize = 10 )

plt.show()

#Importamos el modelo
from sklearn.linear_model import RidgeCV

# Ajustamos el modelo para 100 valores distintos de lambda
n_alphas = 100
alphas = np.logspace(1,5, n_alphas)
ridge = RidgeCV(alphas=alphas, store_cv_values=True).fit(X_train, y_train)

#La funcion RidgeCV no permite controlar la cantidad de k seleccionamos 20 al azar 
number_of_rows = ridge.cv_values_.shape[0]
random_indices = np.random.choice(number_of_rows, 
                                  size=20, 
                                  replace=False)
row = ridge.cv_values_[random_indices, :]

# Graficamos
fig, ax =plt.subplots(figsize=(10,7))

plt.semilogx(ridge.alphas + EPSILON, row.T, ':')
l1 =plt.plot(ridge.alphas + EPSILON,row.T.mean(axis=-1), 'k',
         label='Promedio entre folds', linewidth=2)
ax.axvline(ridge.alpha_ + EPSILON, linestyle='--', color='k',
            label='lambda: CV estimado')

ax.legend(fontsize=10)

ax.set_xlabel(r'$\lambda$', fontsize=10)
ax.set_ylabel('Error Cuadratico Medio', fontsize=10)
ax.set_title('Error cuadratico medio en cada fold: coordinate descent '
          '(train time: %.2fs)' % t_lasso_cv, fontweight='bold', fontsize=20)
plt.xticks(fontsize = 10 )
plt.yticks( fontsize = 10 )

ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)

plt.show()

print(f"El lambda final obtenido mediante CV es: {ridge.alpha_}" )

coeficientes_finales = pd.DataFrame([np.array(X_train.columns.tolist()),ridge.coef_]).T
coeficientes_finales.columns = ['feature','coeficiente']
coeficientes_finales

#Importamos la funcion enet_path
from sklearn.linear_model import enet_path

#Ajsutamos el modelo
eps = 5e-3
alphas_enet, coefs_enet, _ = enet_path(X_train, y_train, eps=eps, l1_ratio=0.5, fit_intercept=False)

#Definimos todos los colores que vamos a utilizar para poder graficar
import matplotlib.pyplot as plt
from matplotlib import colors as mcolors
colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)

# Sort colors by hue, saturation, value and name.
by_hsv = sorted((tuple(mcolors.rgb_to_hsv(mcolors.to_rgba(color)[:3])), name)
                for name, color in colors.items())
sorted_names = [name for hsv, name in by_hsv]
np.random.shuffle(sorted_names)

#Graficamos
eps = 5e-3
fig, ax =plt.subplots(figsize=(10,7))
colors = cycle(sorted_names[:30])

neg_log_alphas_enet = -np.log10(alphas_enet)
for  coef_e, c in zip(coefs_enet, colors):
    l2 = plt.plot(neg_log_alphas_enet, coef_e, linestyle='--', c=c)

ax.set_xlabel('-Log(alpha)',fontsize = 10)
ax.set_ylabel('coefficients',fontsize = 10)
ax.set_title('Elastic-Net Paths', fontweight='bold', fontsize=20)

plt.axis('tight')

plt.xticks(fontsize = 10 )
plt.yticks( fontsize = 10 )

ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
plt.show()

#Importamos el modelo
from sklearn.linear_model import ElasticNetCV

#Ajustamos el modelo
enet = ElasticNetCV(cv=20).fit(X_train, y_train)


#Graficamos
EPSILON = 1e-4


fig, ax =plt.subplots(figsize=(10,7))
plt.semilogx(enet.alphas_ + EPSILON, enet.mse_path_, ':')
l1 =plt.plot(enet.alphas_ + EPSILON, enet.mse_path_.mean(axis=-1), 'k',
         label='Promedio entre folds', linewidth=2)
ax.axvline(enet.alpha_ + EPSILON, linestyle='--', color='k',
            label='lambda: CV estimado')

ax.legend(fontsize=10)

ax.set_xlabel(r'$\lambda$', fontsize=10)
ax.set_ylabel('Error Cuadratico Medio', fontsize=10)
ax.set_title('Error cuadratico medio en cada fold: coordinate descent '
          '(train time: %.2fs)' % t_lasso_cv, fontweight='bold', fontsize=20)
plt.xticks(fontsize = 10 )
plt.yticks( fontsize = 10 )

ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)

plt.show()

print(f"El lambda final obtenido mediante CV es: {enet.alpha_}" )

coeficientes_finales = pd.DataFrame([np.array(X_train.columns.tolist()),enet.coef_]).T
coeficientes_finales.columns = ['feature','coeficiente']
coeficientes_finales

print(f"El modelo final cuenta con : {coeficientes_finales[coeficientes_finales['coeficiente']!=0].shape[0]}",' features' )

#importamos la metrica
from sklearn.metrics import mean_squared_error

# Computamos el ECM tomando como modelo al promedio de los salarios
promedio = np.repeat(y_train.mean(), y_train.shape[0])
mse_promedio = mean_squared_error(y_train, promedio)

#Importamos el modelo de regresion lineal
from sklearn.linear_model import LinearRegression

#Ajustamos el modelo
model_scaled_lineal = LinearRegression()
model_scaled_lineal = model_scaled_lineal.fit(X_train, y_train)

#Obtenemoos las predicciones y el ECM del modelo de regresion lineal
y_pred_lineal = model_scaled_lineal.predict(X_train)
mse_lineal = mean_squared_error(y_train, y_pred_lineal)

#Obtenemos el ECM de la regresion Lasso

y_pred_lasso = lasso.predict(X_train)
mse_lasso = mean_squared_error(y_train, y_pred_lasso)

#Obtenemos el ECM de la regresion Ridge

y_pred_ridge = ridge.predict(X_train)
mse_ridge = mean_squared_error(y_train, y_pred_ridge)

#Obtenemos el ECM de ElasticNet

y_pred_enet = enet.predict(X_train)
mse_enet = mean_squared_error(y_train, y_pred_enet)


#Ordenamos los datos y los acomodamos en un dataframe
modelo = ['promedio', 'lineal','lasso','ridge','enet']
metrica = np.repeat('ecm', 5)
error = [mse_promedio, mse_lineal, mse_lasso, mse_ridge, mse_enet]
columns = ['modelo', 'metrica', 'valor']
data = np.array([modelo, metrica, error]).T
pd.DataFrame(data=data, columns=columns)

# Computamos el ECM tomando como modelo al promedio de los salarios
promedio = np.repeat(y_test.mean(), y_test.shape[0])
mse_promedio = mean_squared_error(y_test, promedio)

#Obtenemoos las predicciones y el ECM del modelo de regresion lineal
y_pred_lineal = model_scaled_lineal.predict(X_test)
mse_lineal = mean_squared_error(y_test, y_pred_lineal)

#Obtenemos el ECM de la regresion Lasso
y_pred_lasso = lasso.predict(X_test)
mse_lasso = mean_squared_error(y_test, y_pred_lasso)

#Obtenemos el ECM de la regresion Ridge
y_pred_ridge = ridge.predict(X_test)
mse_ridge = mean_squared_error(y_test, y_pred_ridge)


#Obtenemos el ECM de ElasticNet
y_pred_enet = enet.predict(X_test)
mse_enet = mean_squared_error(y_test, y_pred_enet)

#Ordenamos los datos y los acomodamos en un dataframe
y_pred_enet = enet.predict(X_test)
mse_enet = mean_squared_error(y_test, y_pred_enet)

modelo = ['promedio', 'lineal','lasso','ridge','enet']
metrica = np.repeat('ecm', 5)
error = [mse_promedio, mse_lineal, mse_lasso, mse_ridge, mse_enet]
columns = ['modelo', 'metrica', 'valor']
data = np.array([modelo, metrica, error]).T
pd.DataFrame(data=data, columns=columns)

# Importamos el modelo
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import  ElasticNet

#Definimos la grilla
parametros = {'alpha':[0.001,0.01,0.1,1,10,100],
               'l1_ratio':[0,0.25,0.5,0.75,1]}

# Definimos el modelo
enet = ElasticNet() 

grid_search = GridSearchCV(estimator = enet, param_grid = parametros, cv = 20, scoring ='neg_mean_squared_error')

#Ajustamos el modelo
start_time = time.time()
grid_search.fit(X_train, y_train)

tiempo_grid = (time.time() - start_time)
print("Tiempo de ejecucion %s segundos ---\n" % tiempo_grid)
print('Mejor combinación de parámetros %s \n'% grid_search.best_params_)
print('Definición del Modelo %s '% grid_search.best_estimator_)

#Obtenemos la combinacion total de parametros
import itertools as it

diccionario_parametros = sorted(parametros)
combinaciones = it.product(*(parametros[Name] for Name in diccionario_parametros))
combinacioes_realizadas = pd.DataFrame(combinaciones, columns=['lambda', 'alpha'])
combinacioes_realizadas

print('Cantidad de combinaciones evaluadas %s '% combinacioes_realizadas.shape[0])

#Mostramos los coeficientes fianles
coeficientes_finales_grid = pd.DataFrame([np.array(X_train.columns.tolist()),grid_search.best_estimator_.coef_]).T
coeficientes_finales_grid.columns = ['feature','coeficiente']
coeficientes_finales_grid

print(f"El modelo final cuenta con : {coeficientes_finales_grid[coeficientes_finales_grid['coeficiente']!=0].shape[0]}",' features' )

#Importamos el modelo y algunas funciones auxiliares
from sklearn.model_selection import RandomizedSearchCV
from sklearn.utils.fixes import loguniform
import scipy.stats as stats

# Definimos los parametros
parametros={'l1_ratio': stats.uniform(0, 1),
              'alpha': loguniform(1e-4, 1e0)}
n_iteraciones = 15
semilla = 2021

#Ajustamos el modelo
start_time = time.time()


enet_random = ElasticNet()
random_search = RandomizedSearchCV(estimator = enet_random, n_iter = n_iteraciones, param_distributions = parametros,
                                   cv = 20, scoring ='neg_mean_squared_error', random_state = semilla )
random_search.fit(X_train, y_train)
tiempo_random = (time.time() - start_time)

print("Tiempo de ejecucion %s segundos ---\n" % tiempo_random)
print('Mejor combinación de parámetros %s \n'% random_search.best_params_)
print('Definición del Modelo %s \n'% random_search.best_estimator_)
print('Combinaciones Evaluadas %s '% n_iteraciones)

#Visualizamos los coeficientes
coeficientes_finales_random = pd.DataFrame([np.array(X_train.columns.tolist()),random_search.best_estimator_.coef_]).T
coeficientes_finales_random.columns = ['feature','coeficiente']
coeficientes_finales_random

print(f"El modelo final cuenta con : {coeficientes_finales_random[coeficientes_finales_random['coeficiente']!=0].shape[0]}",' features' )

#Computamos el ECM del Grid Search
y_pred_grid = grid_search.best_estimator_.predict(X_test)
mse_grid = mean_squared_error(y_test, y_pred_grid)

#Computamos el ECM del Random Search
y_pred_random = random_search.best_estimator_.predict(X_test)
mse_random = mean_squared_error(y_test, y_pred_random)

#Organizamos la informacion en un dataframe
estrategia = ['GridSearch', 'RandomSearch']
tiempo = [tiempo_grid, tiempo_random]
metrica = np.repeat('ecm', 2)
error = [mse_grid, mse_random]
iteraciones = [combinacioes_realizadas.shape[0], n_iteraciones ]
folds = [20,20]
n_variables = [ coeficientes_finales_grid[coeficientes_finales_grid['coeficiente']!=0].shape[0], coeficientes_finales_random[coeficientes_finales_random['coeficiente']!=0].shape[0]]
columns = ['estregia','tiempo', 'metrica', 'valor', 'combinaciones', 'folds', 'n_variables']
data = np.array([estrategia, tiempo, metrica, error, iteraciones, folds, n_variables]).T
pd.DataFrame(data=data, columns=columns)

nba = pd.read_csv('https://datasets-humai.s3.amazonaws.com/datasets/nba_salarios_2021.csv')

#Observamos cuantos datos faltantes hay en cada columna
nba.isnull().sum()

#Generamos un par de nulos en la posicion para poder imputar categoricas
nba['position'] = np.where(nba['salary'].isnull(),np.nan, nba['position'])

# Visualizamos los casos con faltantes
nba[nba['salary'].isnull()]

# Importamos la funcion
from sklearn.impute import SimpleImputer

# Separamos las variables categoricas y nulas con faltantes para poder tener una mejor visualizacion
numericas = nba[['salary']]
categoricas = nba[['position']]


# Imputamos con la media
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(numericas[['salary']])
numericas['media']=imputer.transform(numericas[['salary']]) 

# Imputamos con la mediana
imputer = SimpleImputer(missing_values=np.nan, strategy='median')
imputer.fit(numericas[['salary']])
numericas['mediana']=imputer.transform(numericas[['salary']]) 

# Imputamos con un valor constante
imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value = 99999)
imputer.fit(numericas[['salary']])
numericas['constante']=imputer.transform(numericas[['salary']]) 

# Imputamos con el valor mas frecuente
imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
imputer.fit(numericas[['salary']])
numericas['mas_frecuente']=imputer.transform(numericas[['salary']]) 

# Visualizamos el resultado de las distintas estraegias
numericas[numericas['salary'].isnull()]

# Imputamos con un valor constante
imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value = 'S/D')
imputer.fit(categoricas[['position']])
categoricas['constante']=imputer.transform(categoricas[['position']]) 

# Imputamos con el valor mas frecuente
imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
imputer.fit(categoricas[['position']])
categoricas['mas_frecuente']=imputer.transform(categoricas[['position']]) 

categoricas[categoricas['position'].isnull()]

# Generamos 10 bins
nba['salario_bins'] = pd.qcut(nba['salary'], q = 10)
nba[['salary','salario_bins']]

#Observamos lso valores que puede tomar la variable position
nba.position.value_counts()

#Reemplazamos los valores que representan multiples posiciones
nba['posicion_agrup'] = np.where(nba['position'].isin(['SG','C','PF','SF','PG']),nba['position'], 'Otro')
nba['posicion_agrup'].value_counts()

#realizamos el One-hot Encoding con get_dummies
pd.get_dummies(nba['position'])

#Importamos OneHotEncoder
from sklearn.preprocessing import OneHotEncoder

#Imputamos los datos faltantes porque no debe contener nans
imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
imputer.fit(nba[['position']])
nba['position']=imputer.transform(nba[['position']]) 

#Ajustamos el encoder 
enc = OneHotEncoder(handle_unknown='ignore')
enc.fit(nba[['position']])

#Generamos las dummies
dummies = enc.transform(nba[['position']])
pd.DataFrame(dummies.toarray())

# Calculamos la media de las variables salary y fga_per_g por posicion
nba.groupby('position').agg({'salary':'mean', 'fga_per_g' :'mean'})

# Calculamos la media de las variables salary y fga_per_g por posicion y equipo
nba.groupby(['position','team_id']).agg({'salary':'mean', 'fga_per_g' :'mean'})

# Calculamos la media, el minimo, maximo y la sumo de la variable salary
nba.groupby('position').agg(min_salary=('salary', 'min'), 
                            max_salary=('salary', 'max'),
                            media_salary = ('salary', 'mean'),
                            suma_salary = ('salary', 'sum'))

#generamos un dataframe que sea solo la columna salario
salario = nba[['salary']]

# Vamos a normalizar la variable salario
# Importamos la función
from sklearn.preprocessing import MinMaxScaler

#Ajustamos el escalado
scaler = MinMaxScaler()
scaler.fit(salario[['salary']])

#Realizamos la normalizacion
salario['norm'] = scaler.transform(salario[['salary']])

salario

# Vamos a estandarizar la variable salario
# Importamos la función
from sklearn.preprocessing import StandardScaler

#Ajustamos el escalado
scaler = StandardScaler()
scaler.fit(salario[['salary']])

#Realizamos la estandarización
salario['std'] = scaler.transform(salario[['salary']])

#vemos como quedaron las distitnas alternativas
salario

#Vemos la distribucion
salario.describe()


#Seleccionamos las variables numéricas
nba_numericas = nba.select_dtypes(include=['float64','int64'])
nba_numericas.head()

#Eliminamos los registros que no poseen salario
nba_numericas = nba_numericas[nba_numericas['salary'].notnull()]
# Separamos la variable target de las predictoras
X = nba_numericas.drop(['salary'], axis=1)
y = nba_numericas[['salary']]

# Separamos en train y en test

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=semilla)

# Definimos el diccionario con los pasos que queremos que tenga el pipeline en este caso StandardScaler y ElasticNet

pasos = [('scaler', StandardScaler()), ('enet', ElasticNet())]

#Importamos la funcion Pipeline
from sklearn.pipeline import Pipeline

# Definimos el objeto que va a contener nuestro pipeline
pipeline = Pipeline(pasos)

# Como vamos a ealphajecutar una busqueda de grilla definimos los parametros a evaluar
#debemos poner el nombre del estimador seguido de __ y luego el parametro
parametros = {'enet__alpha':[0.001,0.01,0.1,1,10,100],
               'enet__l1_ratio':[0,0.25,0.5,0.75,1]}

# Instanciamos la búsqueda de grilla pasandole el pipeline que deseamos que ejecute

grid = GridSearchCV(pipeline, param_grid=parametros, cv=20, scoring= 'neg_mean_squared_error')

# Ajustamos nuestro modelo al conjunto de entramiento
grid.fit(X_train, y_train)

#Obtenemos el mejor Score del modelo
print ("score = %3.2f" %(grid.score(X_test,y_test)))

#Imprimimos los mejores parametros
print( grid.best_params_)


#Agregamos un imputador con una estrategia de media
pasos = [('imputer', SimpleImputer(strategy='mean')),('scaler', StandardScaler()), ('enet', ElasticNet())]

# Definimos el objeto que va a contener nuestro pipeline
pipeline = Pipeline(pasos)

# Como vamos a ealphajecutar una busqueda de grilla definimos los parametros a evaluar
#debemos poner el nombre del estimador seguido de __ y luego el parametro
parametros = {'enet__alpha':[0.001,0.01,0.1,1,10,100],
               'enet__l1_ratio':[0,0.25,0.5,0.75,1]}

# Instanciamos la búsqueda de grilla pasandole el pipeline que deseamos que ejecute

grid = GridSearchCV(pipeline, param_grid=parametros, cv=20, scoring= 'neg_mean_squared_error')

# Ajustamos nuestro modelo al conjunto de entramiento
grid.fit(X_train, y_train)
