{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"AiZ1VL4cTMus"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","source":["# PyTorch\n","\n","PyTorch es un **Framework de Deep Learning** de código abierto que se utiliza para desarrollar y entrenar redes neuronales. Es desarrollado principalmente por el grupo de investigación de IA de Facebook. PyTorch se puede usar tanto con Python como con C++. Naturalmente, la interfaz de Python está más pulida. \n","\n","Un Framework de Deep Learning es un conjunto de interfaces, librerías o herramientas que nos permiten construir modelos de aprendizaje profundo de manera más fácil y rápida, sin entrar en los detalles de los algoritmos subyacentes. Proporcionan una forma clara y concisa de definir modelos utilizando una colección de componentes optimizados y prediseñados. Sus principales características son:\n","* Codificación en alto nivel\n","* Cálculo automático de los gradientes\n","* Paralelización de los procesos\n","* Soporte de la comunidad\n","\n","A diferencia de la mayoría de los otros frameworks populares como TensorFlow, que utilizan grafos computacionales estáticos, PyTorch utiliza grafos dinámicos, lo que permite una mayor flexibilidad en la construcción de arquitecturas complejas. \n","\n","Además, Pytorch utiliza conceptos básicos de Python como clases, estructuras y bucles condicionales, que son muy familiares para nuestros ojos y, por lo tanto, mucho más intuitivos de entender. Esto lo hace mucho más simple que otros frameworks como TensorFlow que aportan su propio estilo de programación.\n"],"metadata":{"id":"NJ9S0VipTlFW"}},{"cell_type":"markdown","metadata":{"id":"ooK6TyhfTMuu"},"source":["\n","\n","Introducción a los Tensores de PyTorch\n","===============================\n","\n","Los tensores son la abstracción de datos central en PyTorch. Este notebook interactivo\n"," proporciona una introducción detallada a la clase ``torch.Tensor``.\n","\n","Lo primero es lo primero, importemos el módulo PyTorch. También agregaremos\n","el módulo `math` de Python para facilitar algunos de los ejemplos.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"gDhfvzu6TMuz","executionInfo":{"status":"ok","timestamp":1656628168577,"user_tz":180,"elapsed":3835,"user":{"displayName":"Pablo Marinozi","userId":"04218818404413865365"}}},"outputs":[],"source":["import torch\n","import math"]},{"cell_type":"markdown","metadata":{"id":"eT3GLFLZTMu0"},"source":["Crear tensores\n","----------------\n","\n","La forma más sencilla de crear un tensor es con la llamada ``torch.empty()``:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pseXKEheTMu3","executionInfo":{"status":"ok","timestamp":1656470977743,"user_tz":180,"elapsed":476,"user":{"displayName":"","userId":""}},"outputId":"3796b22d-429c-434c-b6fb-de270f59ef22","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'torch.Tensor'>\n","tensor([[3.5174e-35, 0.0000e+00, 3.3631e-44, 0.0000e+00],\n","        [       nan, 0.0000e+00, 1.1578e+27, 1.1362e+30],\n","        [7.1547e+22, 4.5828e+30, 1.2121e+04, 7.1846e+22]])\n"]}],"source":["x = torch.empty(3, 4)\n","print(type(x))\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"jUCCPgtVTMu4"},"source":["Desempaquemos lo que acabamos de hacer:\n","\n","- Creamos un tensor utilizando uno de los numerosos métodos constructores provistos por ``torch``.\n","- El tensor en sí es bidimensional, tiene 3 filas y 4 columnas.\n","- El tipo de objeto devuelto es ``torch.Tensor``, que es un\n","   alias para ``torch.FloatTensor``; por defecto, los tensores de PyTorch son\n","   poblados con números de punto flotante de 32 bits. \n","- Probablemente verá algunos valores de aspecto aleatorio al imprimir su\n","   tensor. La llamada ``torch.empty()`` asigna memoria para el tensor,\n","   pero no lo inicializa con ningún valor, por lo que lo que está viendo es\n","   lo que estaba en la memoria en el momento de la asignación.\n","\n","Una breve nota sobre los tensores y su número de dimensiones, y\n","terminología:\n","\n","- A veces verás un tensor unidimensional llamado\n","   **vector**\n","- Del mismo modo, un tensor bidimensional a menudo se denomina\n","   **matriz**\n","- Cualquier cosa con más de dos dimensiones generalmente es solo\n","   llamado tensor.\n","\n","La mayoría de las veces, querrá inicializar su tensor con algún\n","valor. Los casos comunes son todos ceros, todos unos o valores aleatorios, y el\n","El módulo ``torch`` proporciona métodos constructores para todos estos:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7dGYpwUpTMu6","executionInfo":{"status":"ok","timestamp":1656471023402,"user_tz":180,"elapsed":484,"user":{"displayName":"","userId":""}},"outputId":"ebc10b2c-4f3f-45bc-8b0c-2349487a76eb","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 0., 0.],\n","        [0., 0., 0.]])\n","tensor([[1., 1., 1.],\n","        [1., 1., 1.]])\n","tensor([[0.3126, 0.3791, 0.3087],\n","        [0.0736, 0.4216, 0.0691]])\n"]}],"source":["zeros = torch.zeros(2, 3)\n","print(zeros)\n","\n","ones = torch.ones(2, 3)\n","print(ones)\n","\n","torch.manual_seed(1729)\n","random = torch.rand(2, 3)\n","print(random)"]},{"cell_type":"markdown","metadata":{"id":"4T2cCWllTMu8"},"source":["Todos los métodos constructores hacen exactamente lo que cabría esperar: tenemos un tensor\n","llena de ceros, otro llena de unos y otro con valores aleatorios\n","entre 0 y 1.\n","\n","### Tensores Aleatorios y Semillas\n","\n","Hablando del tensor aleatorio, ¿notaste la llamada a\n","``torch.manual_seed()`` inmediatamente anterior? Inicializar tensores,\n","como los pesos de aprendizaje de un modelo, con valores aleatorios es común pero\n","hay momentos, especialmente en entornos de investigación, en los que querrá\n","cierta seguridad de la reproducibilidad de sus resultados. Asignar manualmente \n","la semilla de su generador de números aleatorios es la forma de hacer esto. Miremos\n","más cerca:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NC6izNfSTMu9","executionInfo":{"status":"ok","timestamp":1656471381097,"user_tz":180,"elapsed":464,"user":{"displayName":"","userId":""}},"outputId":"acc0bbd5-4e37-4902-edbb-afbb00687611","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.3126, 0.3791, 0.3087],\n","        [0.0736, 0.4216, 0.0691]])\n","tensor([[0.2332, 0.4047, 0.2162],\n","        [0.9927, 0.4128, 0.5938]])\n","tensor([[0.3126, 0.3791, 0.3087],\n","        [0.0736, 0.4216, 0.0691]])\n","tensor([[0.2332, 0.4047, 0.2162],\n","        [0.9927, 0.4128, 0.5938]])\n"]}],"source":["torch.manual_seed(1729)\n","random1 = torch.rand(2, 3)\n","print(random1)\n","\n","random2 = torch.rand(2, 3)\n","print(random2)\n","\n","torch.manual_seed(1729)\n","random3 = torch.rand(2, 3)\n","print(random3)\n","\n","random4 = torch.rand(2, 3)\n","print(random4)"]},{"cell_type":"markdown","metadata":{"id":"7dl8Mg-PTMu_"},"source":["Lo que deberías ver arriba es que ``random1`` y ``random3`` llevan\n","valores idénticos, al igual que ``random2`` y ``random4``. La asignación manual\n","la semilla del generador de numeros aleatorios lo reinicia, de modo que los cálculos idénticos \n","números aleatorios deberían, en la mayoría de los entornos, proporcionar resultados idénticos.\n","\n","Para obtener más información, consulte la documentación de PyTorch sobre\n","reproducibilidad <https://pytorch.org/docs/stable/notes/randomness.html>`__.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","source":["### Formas de los Tensores\n","\n","A menudo, para poder realizar operaciones en dos o más tensores, estos\n","tendrán que tener la misma ***forma***, es decir, tener el mismo número de\n","dimensiones y el mismo número de celdas en cada dimensión. Para garantizar eso existen los métodos ``torch.*_like()``:"],"metadata":{"id":"3wE-qyutZgPj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"b2AT3oquTMvA","executionInfo":{"status":"ok","timestamp":1656471642179,"user_tz":180,"elapsed":575,"user":{"displayName":"","userId":""}},"outputId":"dbd0b22d-19f9-4986-9d87-e7d90a68bddc","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 2, 3])\n","tensor([[[3.5176e-35, 0.0000e+00, 1.5695e-43],\n","         [1.6255e-43, 1.6956e-43, 1.3312e-43]],\n","\n","        [[1.5134e-43, 1.4714e-43, 1.4994e-43],\n","         [1.4153e-43, 1.3312e-43, 1.6816e-43]]])\n","torch.Size([2, 2, 3])\n","tensor([[[3.5176e-35, 0.0000e+00, 3.3631e-44],\n","         [0.0000e+00,        nan, 1.5912e+00]],\n","\n","        [[1.1578e+27, 1.1362e+30, 7.1547e+22],\n","         [4.5828e+30, 1.2121e+04, 7.1846e+22]]])\n","torch.Size([2, 2, 3])\n","tensor([[[0., 0., 0.],\n","         [0., 0., 0.]],\n","\n","        [[0., 0., 0.],\n","         [0., 0., 0.]]])\n","torch.Size([2, 2, 3])\n","tensor([[[1., 1., 1.],\n","         [1., 1., 1.]],\n","\n","        [[1., 1., 1.],\n","         [1., 1., 1.]]])\n","torch.Size([2, 2, 3])\n","tensor([[[0.6128, 0.1519, 0.0453],\n","         [0.5035, 0.9978, 0.3884]],\n","\n","        [[0.6929, 0.1703, 0.1384],\n","         [0.4759, 0.7481, 0.0361]]])\n"]}],"source":["x = torch.empty(2, 2, 3)\n","print(x.shape)\n","print(x)\n","\n","empty_like_x = torch.empty_like(x)\n","print(empty_like_x.shape)\n","print(empty_like_x)\n","\n","zeros_like_x = torch.zeros_like(x)\n","print(zeros_like_x.shape)\n","print(zeros_like_x)\n","\n","ones_like_x = torch.ones_like(x)\n","print(ones_like_x.shape)\n","print(ones_like_x)\n","\n","rand_like_x = torch.rand_like(x)\n","print(rand_like_x.shape)\n","print(rand_like_x)"]},{"cell_type":"markdown","metadata":{"id":"lqzcMLb6TMvB"},"source":["Lo primero nuevo en la celda de código de arriba es el uso del atributo ``.shape``\n","de todo tensor. Este atributo contiene una lista con el tamaño de\n","cada dimensión de un tensor - en nuestro caso, ``x`` es un tensor tridimensional\n"," con forma 2 x 2 x 3.\n","\n","Debajo de eso, llamamos a los métodos ``.empty_like()``, ``.zeros_like()``,\n","``.ones_like()`` y ``.rand_like()``. Usando el atributo ``.shape``, podemos verificar que cada uno de estos métodos devuelve un tensor de\n","dimensionalidad y extensión idénticas.\n","\n","La última forma de crear un tensor es especificar sus datos\n","directamente desde una colección de PyTorch:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ESchpYJQTMvC","executionInfo":{"status":"ok","timestamp":1656472132534,"user_tz":180,"elapsed":453,"user":{"displayName":"","userId":""}},"outputId":"8445574e-7ebf-4b8b-93de-5dbd96c3488b","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[3.1416, 2.7183],\n","        [1.6180, 0.0073]])\n","tensor([ 2,  3,  5,  7, 11, 13, 17, 19])\n","tensor([[2, 4, 6],\n","        [3, 6, 9]])\n"]}],"source":["some_constants = torch.tensor([[3.1415926, 2.71828], [1.61803, 0.0072897]])\n","print(some_constants)\n","\n","some_integers = torch.tensor((2, 3, 5, 7, 11, 13, 17, 19))\n","print(some_integers)\n","\n","more_integers = torch.tensor(((2, 4, 6), [3, 6, 9]))\n","print(more_integers)"]},{"cell_type":"markdown","source":["Usar ``torch.tensor()`` es la forma más sencilla de crear un\n","tensor si ya tiene datos en una tupla o lista de Python. Como se muestra\n","anterior, el anidamiento de las colecciones dará como resultado un tensor multidimensional.\n"],"metadata":{"id":"3P_2n1FYa1_Q"}},{"cell_type":"markdown","metadata":{"id":"E_SbqrakTMvC"},"source":["### Tipos de datos de un Tensor\n","\n","Establecer el tipo de datos de un tensor es posible de dos maneras:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pMhBQyZKTMvD","executionInfo":{"status":"ok","timestamp":1656471985546,"user_tz":180,"elapsed":514,"user":{"displayName":"","userId":""}},"outputId":"18fce2bb-1bcd-44b1-ab20-e5c37e3cce72","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 1, 1],\n","        [1, 1, 1]], dtype=torch.int16)\n","tensor([[ 0.9956,  1.4148,  5.8364],\n","        [11.2406, 11.2083, 11.6692]], dtype=torch.float64)\n","tensor([[ 0,  1,  5],\n","        [11, 11, 11]], dtype=torch.int32)\n"]}],"source":["a = torch.ones((2, 3), dtype=torch.int16)\n","print(a)\n","\n","b = torch.rand((2, 3), dtype=torch.float64) * 20.\n","print(b)\n","\n","c = b.to(torch.int32)\n","print(c)"]},{"cell_type":"markdown","metadata":{"id":"Rbe0ZGSmTMvD"},"source":["La forma más sencilla de establecer el tipo de datos subyacente de un tensor es con un\n","argumento opcional en el momento de la creación. En la primera línea de la celda de arriba,\n","configuramos ``dtype=torch.int16`` para el tensor ``a``. Cuando imprimimos ``a``,\n","podemos ver que está lleno de ``1`` en lugar de ``1.`` - La sutileza de Python\n","indica que este es un tipo entero en lugar de un punto flotante.\n","\n","Otra cosa a tener en cuenta sobre la impresión de ``a`` es que, a diferencia de cuando\n","dejó ``dtype`` como predeterminado (coma flotante de 32 bits), imprimiendo el\n","tensor también especifica su ``dtype``.\n","\n","Es posible que también haya notado que pasamos de especificar la forma del tensor como una serie de argumentos enteros, a agrupar esos argumentos en un\n","tupla. Esto no es estrictamente necesario: PyTorch tomará una serie de\n","argumentos enteros iniciales sin etiquetar como la forma del tensor, pero al agregar\n","los argumentos opcionales, ponerlos como tupla puede hacer que su intención sea más legible.\n","\n","La otra forma de establecer el tipo de datos es con el método ``.to()``. En la\n","celda de arriba, creamos un tensor de punto flotante aleatorio ``b`` de la manera habitual. A continuación, creamos ``c`` convirtiendo ``b`` en un tensor entero  de 32 bits\n","con el método ``.to()``. Tenga en cuenta que ``c`` contiene todos los mismos\n","valores como ``b``, pero truncados a enteros.\n","\n","Los tipos de datos disponibles incluyen:\n","\n","-  ``torch.bool``\n","-  ``torch.int8``\n","-  ``torch.uint8``\n","-  ``torch.int16``\n","-  ``torch.int32``\n","-  ``torch.int64``\n","-  ``torch.half``\n","-  ``torch.float``\n","-  ``torch.double``\n","-  ``torch.bfloat``\n","\n","Matemáticas y lógica con tensores PyTorch\n","---------------------------------\n","\n","Ahora que conoces algunas de las formas de crear un tensor… ¿qué puedes hacer con ellos?\n","\n","Veamos primero la aritmética básica y cómo interactúan los tensores con\n","escalares simples:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1uq-CTiwTMvE","executionInfo":{"status":"ok","timestamp":1656472413046,"user_tz":180,"elapsed":479,"user":{"displayName":"","userId":""}},"outputId":"e42409ad-b14b-4e6c-893e-8c05110cf8ef","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1.],\n","        [1., 1.]])\n","tensor([[2., 2.],\n","        [2., 2.]])\n","tensor([[3., 3.],\n","        [3., 3.]])\n","tensor([[4., 4.],\n","        [4., 4.]])\n","tensor([[1.4142, 1.4142],\n","        [1.4142, 1.4142]])\n"]}],"source":["ones = torch.zeros(2, 2) + 1\n","twos = torch.ones(2, 2) * 2\n","threes = (torch.ones(2, 2) * 7 - 1) / 2\n","fours = twos ** 2\n","sqrt2s = twos ** 0.5\n","\n","print(ones)\n","print(twos)\n","print(threes)\n","print(fours)\n","print(sqrt2s)"]},{"cell_type":"markdown","metadata":{"id":"aJ3sfMEbTMvE"},"source":["\n","Como puedes ver arriba, las operaciones aritméticas entre tensores y escalares,\n","como suma, resta, multiplicación, división y\n","exponenciación se aplican elemento a elemento dentro del tensor. Dado que\n","la salida de tal operación será un tensor, puedes encadenarlos\n","junto con las reglas usuales de precedencia de operadores, como en la línea donde\n","creamos ``threes``.\n","\n","Las operaciones similares entre dos tensores también se comportan intuitivamente:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JvBH68BcTMvF","executionInfo":{"status":"ok","timestamp":1656472532008,"user_tz":180,"elapsed":464,"user":{"displayName":"","userId":""}},"outputId":"49337a80-fcb7-4d28-920a-95cb66f99d0d","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 2.,  4.],\n","        [ 8., 16.]])\n","tensor([[5., 5.],\n","        [5., 5.]])\n","tensor([[12., 12.],\n","        [12., 12.]])\n"]}],"source":["powers2 = twos ** torch.tensor([[1, 2], [3, 4]])\n","print(powers2)\n","\n","fives = ones + fours\n","print(fives)\n","\n","dozens = threes * fours\n","print(dozens)"]},{"cell_type":"markdown","metadata":{"id":"idZ8bv7mTMvG"},"source":["Es importante notar aquí que todos los tensores en la celda de código anterior\n"," eran de forma idéntica. ¿Qué sucede cuando tratamos de realizar una operación binaria entre tensores si la forma es diferente?\n","\n","<div class=\"alert alert-info\"><h4>Nota</h4><p>La siguiente celda arroja un error de tiempo de ejecución. Esto es intencional.</p></div>"]},{"cell_type":"code","source":["   a = torch.rand(2, 3)\n","   b = torch.rand(3, 2)\n","\n","   print(a * b)"],"metadata":{"id":"jUYwzP1Ydk2o","executionInfo":{"status":"error","timestamp":1656472689187,"user_tz":180,"elapsed":488,"user":{"displayName":"","userId":""}},"outputId":"b9c2de58-3a56-4062-e118-2dfcc752b91c","colab":{"base_uri":"https://localhost:8080/","height":221}},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-fcc83145fe91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"]}]},{"cell_type":"markdown","metadata":{"id":"2OxFMHIaTMvH"},"source":["### Broadcasting de Tensores\n","\n","En el caso general, no se puede operar con tensores de diferente forma\n","de esta manera, incluso en un caso como el de la celda anterior, donde los tensores tienen un\n","idéntico número de elementos.\n","\n","<div class=\"alert alert-info\"><h4>Nota</h4><p>Si está familiarizado con el broadcasting de NumPy, aquí se aplican las mismas reglas.</p></div>\n","\n","La excepción a la regla de las mismas formas es el ***broadcasting de tensores***. Aquí hay\n","un ejemplo:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eRs-H_iLTMvJ","executionInfo":{"status":"ok","timestamp":1656462365368,"user_tz":180,"elapsed":445,"user":{"displayName":"","userId":""}},"outputId":"4a28f81d-0a82-4e46-ce22-947599c289e7","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2024, 0.5731, 0.7191, 0.4067],\n","        [0.7301, 0.6276, 0.7357, 0.0381]])\n","tensor([[0.4049, 1.1461, 1.4382, 0.8134],\n","        [1.4602, 1.2551, 1.4715, 0.0762]])\n"]}],"source":["rand = torch.rand(2, 4)\n","doubled = rand * (torch.ones(1, 4) * 2)\n","\n","print(rand)\n","print(doubled)"]},{"cell_type":"markdown","metadata":{"id":"jRYOWNgUTMvL"},"source":["¿Cuál es el truco aquí? ¿Cómo es que podemos multiplicar un tensor de 2x4 por un\n","tensor 1x4?\n","\n","El broadcasting es una forma de realizar una operación entre tensores que tienen\n","similitudes en sus formas. En el ejemplo anterior,\n","el tensor de cuatro columnas, el de una fila, se multiplica por *ambas filas* del de dos filas,\n","tensor de cuatro columnas.\n","\n","Esta es una operación importante en Deep Learning. El ejemplo común es\n","multiplicar un tensor de pesos de aprendizaje por un *lote* de tensores de entrada,\n","aplicando la operación a cada instancia en el lote por separado, y\n","devolviendo un tensor de forma idéntica, al igual que nuestro (2, 4) \\* (1, 4)\n","ejemplo anterior devolvió un tensor de forma (2, 4).\n","\n","Las reglas para el broadcasting son:\n","\n","- Cada tensor debe tener al menos una dimensión - no hay tensores vacíos.\n","\n","- Comparando los tamaños de las dimensiones de los dos tensores, *yendo del último al\n","   primero:*\n","\n","   - Cada dimensión debe ser igual, *o*\n","\n","   - Una de las dimensiones debe ser de tamaño 1, *o*\n","\n","   - La dimensión no existe en uno de los tensores\n","\n","Los tensores de forma idéntica, por supuesto, son trivialmente \"bradcasteables\", como\n","viste antes.\n","\n","Aquí hay algunos ejemplos de situaciones que respetan las reglas anteriores y\n","permiten el bradcasting:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jWqxyEh1TMvM"},"outputs":[],"source":["a =     torch.ones(4, 3, 2)\n","\n","b = a * torch.rand(   3, 2) # 3ra & 2da dims identicas a las de a, dim 1 ausente\n","print(b)\n","\n","c = a * torch.rand(   3, 1) # 3ra dim = 1, 2da dim identica a la de a\n","print(c)\n","\n","d = a * torch.rand(   1, 2) # 3ra dim identica a la de a, 2da dim = 1\n","print(d)"]},{"cell_type":"markdown","metadata":{"id":"VfS91ltQTMvO"},"source":["Para obtener más información sobre el broadcasting, consulte la documentación de PyTorch\n"," <https://pytorch.org/docs/stable/notes/broadcasting.html>`\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zaQyonB-TMvO"},"source":["### Más matemáticas con tensores\n","\n","Los tensores PyTorch tienen más de trescientas operaciones que se pueden realizar\n","en ellos.\n","\n","Aquí hay una pequeña muestra de algunas de las principales categorías de operaciones:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KVrRYPGtTMvO"},"outputs":[],"source":["# common functions\n","a = torch.rand(2, 4) * 2 - 1\n","print('Common functions:')\n","print(torch.abs(a))\n","print(torch.ceil(a))\n","print(torch.floor(a))\n","print(torch.clamp(a, -0.5, 0.5))\n","\n","# trigonometric functions and their inverses\n","angles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n","sines = torch.sin(angles)\n","inverses = torch.asin(sines)\n","print('\\nSine and arcsine:')\n","print(angles)\n","print(sines)\n","print(inverses)\n","\n","# bitwise operations\n","print('\\nBitwise XOR:')\n","b = torch.tensor([1, 5, 11])\n","c = torch.tensor([2, 7, 10])\n","print(torch.bitwise_xor(b, c))\n","\n","# comparisons:\n","print('\\nBroadcasted, element-wise equality comparison:')\n","d = torch.tensor([[1., 2.], [3., 4.]])\n","e = torch.ones(1, 2)  # many comparison ops support broadcasting!\n","print(torch.eq(d, e)) # returns a tensor of type bool\n","\n","# reductions:\n","print('\\nReduction ops:')\n","print(torch.max(d))        # returns a single-element tensor\n","print(torch.max(d).item()) # extracts the value from the returned tensor\n","print(torch.mean(d))       # average\n","print(torch.std(d))        # standard deviation\n","print(torch.prod(d))       # product of all numbers\n","print(torch.unique(torch.tensor([1, 2, 1, 2, 1, 2]))) # filter unique elements\n","\n","# vector and linear algebra operations\n","v1 = torch.tensor([1., 0., 0.])         # x unit vector\n","v2 = torch.tensor([0., 1., 0.])         # y unit vector\n","m1 = torch.rand(2, 2)                   # random matrix\n","m2 = torch.tensor([[3., 0.], [0., 3.]]) # three times identity matrix\n","\n","print('\\nVectors & Matrices:')\n","print(torch.cross(v2, v1)) # negative of z unit vector (v1 x v2 == -v2 x v1)\n","print(m1)\n","m3 = torch.matmul(m1, m2)\n","print(m3)                  # 3 times m1\n","print(torch.svd(m3))       # singular value decomposition"]},{"cell_type":"markdown","source":["Esta es una pequeña muestra de operaciones. Para más detalles y el inventario completo de\n","funciones matemáticas, echa un vistazo a la\n","`documentación` <https://pytorch.org/docs/stable/torch.html#math-operations>`__."],"metadata":{"id":"8dWRJ8Mj4j_8"}},{"cell_type":"markdown","metadata":{"id":"g-ZsXeZLTMvQ"},"source":["\n","\n","### Alteración de tensores en su lugar\n","\n","\n","La mayoría de las operaciones binarias entre tensores devolverán un tercer tensor nuevo. Cuando\n","decimos ``c = a * b`` (donde ``a`` y ``b`` son tensores), el nuevo tensor\n","``c`` ocupará una región de memoria distinta de los otros tensores.\n","\n","Sin embargo, hay ocasiones en las que es posible que desee alterar un tensor en su lugar:\n","por ejemplo, si está haciendo un cálculo por elementos en el que puede\n","descartar valores intermedios. Para esto, la mayoría de las funciones matemáticas tienen un\n","versión con un guión bajo adjunto (``_``) que alterará un tensor en su\n","lugar.\n","\n","Por ejemplo:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nr9t_DyzTMvR"},"outputs":[],"source":["a = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n","print('a:')\n","print(a)\n","print(torch.sin(a))   # esta operación crea un nuevo tensor en la memoria\n","print(a)              # a no ha cambiado \n","\n","b = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n","print('\\nb:')\n","print(b)\n","print(torch.sin_(b))  # note el guión bajo\n","print(b)              # b ha cambiado"]},{"cell_type":"markdown","metadata":{"id":"sOrdae34TMvR"},"source":["Para las operaciones aritméticas, existen funciones que se comportan de manera similar:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IJYc2E6fTMvR"},"outputs":[],"source":["a = torch.ones(2, 2)\n","b = torch.rand(2, 2)\n","\n","print('Before:')\n","print(a)\n","print(b)\n","print('\\nAfter adding:')\n","print(a.add_(b))\n","print(a)\n","print(b)\n","print('\\nAfter multiplying')\n","print(b.mul_(b))\n","print(b)"]},{"cell_type":"markdown","metadata":{"id":"mp8drNSzTMvS"},"source":["Tenga en cuenta que estas funciones aritméticas in situ son métodos del objeto ``torch.Tensor``, no adjunto al módulo ``torch`` como muchos\n","otras funciones (por ejemplo, ``torch.sin()``). Como puedes ver desde\n","``a.add_(b)``, *el tensor de llamada es el que se cambia en\n","lugar.*\n","\n","Existe otra opción para colocar el resultado de un cálculo en un\n","tensor asignado existente. Muchos de los métodos y funciones que hemos visto\n","hasta ahora, ¡incluidos los métodos constructores! , tienen un argumento ``out`` que\n","le permite especificar un tensor para recibir la salida. Si el tensor ``out``\n","es de la forma correcta y ``dtype`` correcto, esto puede suceder sin una nueva asignación de memoria:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4rEB4m4PTMvS"},"outputs":[],"source":["a = torch.rand(2, 2)\n","b = torch.rand(2, 2)\n","c = torch.zeros(2, 2)\n","old_id = id(c)\n","\n","print(c)\n","d = torch.matmul(a, b, out=c)\n","print(c)                # el contenido de c ha cambiado\n","\n","assert c is d           # se fija si c & d son el mismo objeto, no que solo contienen los mismos valore\n","assert id(c), old_id    # se asegura que el nuevo c sea el mismo que el viejo\n","\n","torch.rand(2, 2, out=c) # funciona también para constructores\n","print(c)                # c ha cambiado nuevamente\n","assert id(c), old_id    # todavía es el mismo objeto"]},{"cell_type":"markdown","metadata":{"id":"MURJjaoLTMvS"},"source":["Copiando tensores\n","---------------\n","\n","Como con cualquier objeto en Python, asignar un tensor a una variable convierte a la variable en una *etiqueta* del tensor y no la copia. Por ejemplo:\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"X4WcXXOlTMvT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656628169743,"user_tz":180,"elapsed":9,"user":{"displayName":"Pablo Marinozi","userId":"04218818404413865365"}},"outputId":"e00ef7b1-ad55-49da-e44e-0682f151038c"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[  1., 561.],\n","        [  1.,   1.]])\n"]}],"source":["a = torch.ones(2, 2)\n","b = a\n","\n","a[0][1] = 561  # al cambiar a\n","print(b)       # ...b también se altera"]},{"cell_type":"markdown","metadata":{"id":"U2pchi__TMvT"},"source":["Pero, ¿qué sucede si deseas una copia separada de los datos para trabajar? El método ``clone()`` está ahí para ti:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bkWDLf_lTMvT"},"outputs":[],"source":["a = torch.ones(2, 2)\n","b = a.clone()\n","\n","assert b is not a      # different objects in memory...\n","print(torch.eq(a, b))  # ...but still with the same contents!\n","\n","a[0][1] = 561          # a changes...\n","print(b)               # ...but b is still all ones"]},{"cell_type":"markdown","metadata":{"id":"egK6PB2hTMvZ"},"source":["Manipulación de la forma del tensor\n","--------------------------\n","\n","A veces, necesitarás cambiar la forma de tu tensor. A continuación, veremos algunos casos comunes y cómo manejarlos.\n","\n","###Cambiar el número de dimensiones\n","\n","\n","Un caso en el que podría necesitar cambiar la cantidad de dimensiones es pasar una sola instancia como entrada a su modelo. Los modelos de PyTorch generalmente esperan *lotes* de entrada.\n","\n","Por ejemplo, imagine tener un modelo que funcione con imágenes de 3 x 226 x 226, un cuadrado de 226 píxeles con 3 canales de color. Cuando lo cargues y lo transformes, obtendrás un tensor de forma ``(3, 226, 226)``. Sin embargo, su modelo espera una entrada de forma ``(N, 3, 226, 226)``, donde ``N`` es el número de imágenes en el lote. Entonces, ¿cómo se hace un lote de uno?\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5HEXibbvTMvZ"},"outputs":[],"source":["a = torch.rand(3, 226, 226)\n","b = a.unsqueeze(0)\n","\n","print(a.shape)\n","print(b.shape)"]},{"cell_type":"markdown","metadata":{"id":"0HVDvY7wTMvb"},"source":["El método ``unsqueeze()`` agrega una dimensión de extensión 1.\n","``unsqueeze(0)`` lo agrega como una nueva dimensión cero - ¡ahora tienes un lote de uno!\n","\n","Estamos aprovechando el hecho de que cualquier dimensión de extensión 1 *no* cambia el número de elementos en el tensor.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ZZVNtYHTMvb"},"outputs":[],"source":["c = torch.rand(1, 1, 1, 1, 1)\n","print(c)"]},{"cell_type":"markdown","metadata":{"id":"RjZjbTrSTMvc"},"source":["Continuando con el ejemplo anterior, digamos que la salida del modelo es un vector de 20 elementos para cada entrada. Entonces esperaría que la salida tuviera la forma ``(N, 20)``, donde ``N`` es el número de instancias en el lote de entrada. Eso significa que para nuestro lote de entrada única, obtendremos una salida de forma ``(1, 20)``.\n","\n","¿Qué sucede si desea realizar un cálculo *no por lotes* con esa salida, algo que solo espera un vector de 20 elementos?\n","\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"gewoOUXFTMvc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656618397013,"user_tz":180,"elapsed":537,"user":{"displayName":"Pablo Marinozi","userId":"04218818404413865365"}},"outputId":"21c33730-e3a6-4451-b338-3c6ca7c3f675"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 20])\n","tensor([[0.5052, 0.4619, 0.9835, 0.5341, 0.5414, 0.7336, 0.8064, 0.4359, 0.1039,\n","         0.7360, 0.5491, 0.8013, 0.2305, 0.6235, 0.4822, 0.3123, 0.0474, 0.1107,\n","         0.5544, 0.5571]])\n","torch.Size([20])\n","tensor([0.5052, 0.4619, 0.9835, 0.5341, 0.5414, 0.7336, 0.8064, 0.4359, 0.1039,\n","        0.7360, 0.5491, 0.8013, 0.2305, 0.6235, 0.4822, 0.3123, 0.0474, 0.1107,\n","        0.5544, 0.5571])\n","torch.Size([2, 2])\n","torch.Size([2, 2])\n"]}],"source":["a = torch.rand(1, 20)\n","print(a.shape)\n","print(a)\n","\n","b = a.squeeze(0)\n","print(b.shape)\n","print(b)\n","\n","c = torch.rand(2, 2)\n","print(c.shape)\n","\n","d = c.squeeze(0)\n","print(d.shape)"]},{"cell_type":"markdown","source":["Puede ver en las formas que nuestro tensor bidimensional ahora es\n","1-dimensional, y si miras de cerca la salida de la celda de arriba\n","verás que imprimir ``a`` muestra un conjunto \"extra\" de corchetes\n","``[]`` debido a que tiene una dimensión adicional.\n","\n","Solo puede hacer ``squeeze()`` sobre las dimensiones de tamaño 1. Vea arriba, donde tratamos de comprimir una dimensión de tamaño 2 en ``c``, y terminamos recuperando la misma forma con la que comenzamos. Las llamadas a ``squeeze()`` y ``unsqueeze()`` solo pueden actuar en dimensiones de tamaño 1 porque, de lo contrario, cambiaría el número de elementos en el tensor."],"metadata":{"id":"fWd8MRbmJ9ez"}},{"cell_type":"markdown","source":["Otro lugar en el que podrías usar ``unsqueeze()`` es para facilitar el broadcasting. Recuerde el ejemplo anterior donde teníamos el siguiente código:"],"metadata":{"id":"L7uLV1mFKXds"}},{"cell_type":"code","source":["a = torch.ones(4, 3, 2)\n","\n","c = a * torch.rand(   3, 1) # 3rd dim = 1, 2nd dim identical to a\n","print(c)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hD7diDJcKZE3","executionInfo":{"status":"ok","timestamp":1656618618890,"user_tz":180,"elapsed":473,"user":{"displayName":"Pablo Marinozi","userId":"04218818404413865365"}},"outputId":"ac8adaa9-2177-415c-e383-a7449910732c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[0.7708, 0.7708],\n","         [0.7431, 0.7431],\n","         [0.0831, 0.0831]],\n","\n","        [[0.7708, 0.7708],\n","         [0.7431, 0.7431],\n","         [0.0831, 0.0831]],\n","\n","        [[0.7708, 0.7708],\n","         [0.7431, 0.7431],\n","         [0.0831, 0.0831]],\n","\n","        [[0.7708, 0.7708],\n","         [0.7431, 0.7431],\n","         [0.0831, 0.0831]]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"7P7ue18ETMvc"},"source":["El efecto neto de eso fue operar con broadcast sobre las dimensiones 0 y 2, lo que provocó que el tensor aleatorio de 3 x 1 se multiplicara elemento a elemento por cada columna de 3 elementos en ``a``.\n","\n","¿Qué pasaría si el vector aleatorio hubiera sido un vector de 3 elementos? Perderíamos la capacidad de hacer broadcasting, porque las dimensiones finales no coincidirían de acuerdo con las reglas del broadcasting. ``unsqueeze()`` viene al rescate:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MnpPtHRtTMvd"},"outputs":[],"source":["a = torch.ones(4, 3, 2)\n","b = torch.rand(   3)     # intentar multiplicar a * b dará un error de tiempo de ejecución\n","c = b.unsqueeze(1)       # cambiar a un tensor bidimensional, agregando un nuevo dim al final\n","print(c.shape)\n","print(a * c)             # ¡El broadcast funciona de nuevo!"]},{"cell_type":"markdown","metadata":{"id":"k2FlCeTATMvd"},"source":["Los métodos squeeze() y unsqueeze() también tienen versiones in situ, squeeze_() y unsqueeze_():\n","\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"GJmCHqJqTMve","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656619029471,"user_tz":180,"elapsed":462,"user":{"displayName":"Pablo Marinozi","userId":"04218818404413865365"}},"outputId":"000b67cd-32f4-47cc-de63-136f4950bfeb"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 226, 226])\n","torch.Size([1, 3, 226, 226])\n"]}],"source":["batch_me = torch.rand(3, 226, 226)\n","print(batch_me.shape)\n","batch_me.unsqueeze_(0)\n","print(batch_me.shape)"]},{"cell_type":"markdown","metadata":{"id":"IOzhnOogTMve"},"source":["A veces querrá cambiar la forma de un tensor de forma más radical, conservando al mismo tiempo la cantidad de elementos y su contenido. \n","\n","``reshape()`` hará esto por ti, siempre que las dimensiones que solicites produzcan el mismo número de elementos que tiene el tensor de entrada:\n","\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"cYBWDEBRTMve","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656619177295,"user_tz":180,"elapsed":473,"user":{"displayName":"Pablo Marinozi","userId":"04218818404413865365"}},"outputId":"aa168dc3-ccc2-450f-ed01-0ae046998022"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([6, 20, 20])\n","torch.Size([2400])\n","torch.Size([2400])\n"]}],"source":["output3d = torch.rand(6, 20, 20)\n","print(output3d.shape)\n","\n","input1d = output3d.reshape(6 * 20 * 20)\n","print(input1d.shape)\n","\n","# can also call it as a method on the torch module:\n","print(torch.reshape(output3d, (6 * 20 * 20,)).shape)"]},{"cell_type":"markdown","metadata":{"id":"MeQRf1jGTMvf"},"source":["El argumento ``(6 * 20 * 20,)`` en la línea final de la celda anterior se debe a que PyTorch espera una **tupla** al especificar una forma de tensor, pero cuando la forma es el primer argumento de un método, nos permite hacer trampa y simplemente usar una serie de números enteros. Aquí, tuvimos que agregar los paréntesis y la coma para convencer al método de que se trata realmente de una tupla de un elemento.\n","\n","Cuando pueda, ``reshape()`` devolverá una *vista* del tensor a ser\n","cambiado, es decir, un objeto tensor separado que mira la misma región subyacente de la memoria. *Esto es importante:* Eso significa que cualquier cambio realizado en el tensor fuente se reflejará en la vista de ese tensor, a menos que le hagas ``clone()``.\n","\n","Para obtener más información, consulte la\n","documentación <https://pytorch.org/docs/stable/torch.html#torch.reshape>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AX3g_hRtTMvf"},"source":["Puente con NumPy\n","------------\n","\n","En la sección anterior sobre broadcasting, se mencionó que la semántica de broadcasting de PyTorch es compatible con la de NumPy, pero la afinidad entre PyTorch y NumPy es aún más profunda que eso.\n","\n","Si tiene código científico o de ML pre-existente con datos almacenados en NumPy ndarrays, es posible que desee expresar esos mismos datos como tensores PyTorch, ya sea para aprovechar la aceleración GPU de PyTorch o sus abstracciones eficientes para construir modelos neuronales. \n","\n","Pues es fácil cambiar entre ndarrays y tensores PyTorch:\n","\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"V1WMPpyvTMvf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656619872149,"user_tz":180,"elapsed":471,"user":{"displayName":"Pablo Marinozi","userId":"04218818404413865365"}},"outputId":"67002ef3-53c3-473e-f929-16a71fbe4e7b"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[1. 1. 1.]\n"," [1. 1. 1.]]\n","tensor([[1., 1., 1.],\n","        [1., 1., 1.]], dtype=torch.float64)\n"]}],"source":["import numpy as np\n","\n","numpy_array = np.ones((2, 3))\n","print(numpy_array)\n","\n","pytorch_tensor = torch.from_numpy(numpy_array)\n","print(pytorch_tensor)"]},{"cell_type":"markdown","metadata":{"id":"A5OzNOIxTMvf"},"source":["PyTorch crea un tensor de la misma forma y que contiene los mismos datos que los arreglos NumPy, llegando incluso a mantener el tipo de datos flotante de 64 bits predeterminado de NumPy.\n","\n","La conversión puede ir fácilmente a la inversa:\n","\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"1EWg0MuqTMvg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656619878393,"user_tz":180,"elapsed":521,"user":{"displayName":"Pablo Marinozi","userId":"04218818404413865365"}},"outputId":"f1fc85a8-2467-4da2-ca9d-2080c98b2e3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2461, 0.4066, 0.0241],\n","        [0.2464, 0.3896, 0.7089]])\n","[[0.24610507 0.40662938 0.02413321]\n"," [0.24637604 0.3895728  0.7088829 ]]\n"]}],"source":["pytorch_rand = torch.rand(2, 3)\n","print(pytorch_rand)\n","\n","numpy_rand = pytorch_rand.numpy()\n","print(numpy_rand)"]},{"cell_type":"markdown","metadata":{"id":"lfLR2tBaTMvg"},"source":["Es importante saber que estos objetos convertidos utilizan *la misma memoria subyacente* que sus objetos de origen, lo que significa que los cambios en uno se reflejan en el otro:\n","\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"NUVqJqOMTMvg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656619916962,"user_tz":180,"elapsed":493,"user":{"displayName":"Pablo Marinozi","userId":"04218818404413865365"}},"outputId":"4469c095-8cca-47aa-fac8-0ba22b018264"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.,  1.,  1.],\n","        [ 1., 23.,  1.]], dtype=torch.float64)\n","[[ 0.24610507  0.40662938  0.02413321]\n"," [ 0.24637604 17.          0.7088829 ]]\n"]}],"source":["numpy_array[1, 1] = 23\n","print(pytorch_tensor)\n","\n","pytorch_rand[1, 1] = 17\n","print(numpy_rand)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"colab":{"name":"clase1_2 Tensores","provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/2d065e41c3445129d492b59662c6dd8c/tensors_deeper_tutorial.ipynb","timestamp":1656463316114}],"toc_visible":true,"collapsed_sections":["E_SbqrakTMvC","2OxFMHIaTMvH","zaQyonB-TMvO","g-ZsXeZLTMvQ"]}},"nbformat":4,"nbformat_minor":0}