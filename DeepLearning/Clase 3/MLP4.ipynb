{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLP4.ipynb","provenance":[{"file_id":"1rz7h-TOsFd-WNlYrIxiu8ICaMarFjDpY","timestamp":1656721458325}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch"],"metadata":{"id":"BUFa4jBEBj4A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Autograd durante el entrenamiento\n","--------------------\n","\n","Hemos echado un breve vistazo a cómo funciona Autograd, pero ¿cómo se ve cuando se usa para el propósito previsto? Definamos un modelo pequeño y examinemos cómo cambia después de un solo lote de entrenamiento. Primero, definimos algunas constantes, nuestro modelo y algunos sustitutos para entradas y salidas:"],"metadata":{"id":"QZaxSTSOXO85"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dp8VeM0uEDXx"},"outputs":[],"source":["BATCH_SIZE = 16\n","DIM_IN = 784\n","HIDDEN_SIZE = 256\n","DIM_OUT = 10\n","\n","net = torch.nn.Sequential(torch.nn.Linear(DIM_IN, HIDDEN_SIZE),\n","                    torch.nn.ReLU(),\n","                    torch.nn.Linear(HIDDEN_SIZE, DIM_OUT))\n","\n","# features aleatorias\n","some_input = torch.randn(BATCH_SIZE, DIM_IN, requires_grad=False)\n","# etiquetas aleatorias\n","ideal_output = torch.randn(BATCH_SIZE, DIM_OUT, requires_grad=False)\n","\n","model = net"]},{"cell_type":"markdown","metadata":{"id":"3Iq_1R_DQTQn"},"source":["Fijemos nos que no hizo falta agregar\n","``requires_grad=True`` a las capas de modelo esto es por que la clase ``torch.nn.Module`` supone que siempre usaremos el gradiente para entrenar el modelo\n","\n","Sin embargo, al momento de inicial los valores del modelo, el gradiente no se calcula, hasta que lo pidamos.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ck18sjWnQTQo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657055833913,"user_tz":180,"elapsed":26,"user":{"displayName":"Luciano Robino","userId":"12024044421550105553"}},"outputId":"3b292e1a-1e70-4cf5-f85d-c7cdb81a5196"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 0.0034,  0.0084,  0.0573, -0.0133, -0.0443, -0.0521, -0.0101, -0.0125,\n","        -0.0418,  0.0263], grad_fn=<SliceBackward0>)\n","None\n"]}],"source":["print(model[2].weight[0][0:10]) # solo algunos son mostrados\n","print(model[2].weight.grad)"]},{"cell_type":"markdown","metadata":{"id":"FczXgOYkQTQo"},"source":["Veamos que ocurre ahora si entrenamos. \n","\n","Consideremos como función de perdida la distancia cuadrática media entre nuestra ``prediction`` y las etiquetas, ``ideal_output``\n","\n","En este caso usaremos SGD como algoritmos de optimización.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ISiSS0cnQTQr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657055833913,"user_tz":180,"elapsed":19,"user":{"displayName":"Luciano Robino","userId":"12024044421550105553"}},"outputId":"cedf7c62-80d2-42ea-a700-024a39873593"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(174.4287, grad_fn=<SumBackward0>)\n"]}],"source":["optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n","\n","prediction = model(some_input)\n","\n","loss = (ideal_output - prediction).pow(2).sum()\n","print(loss)"]},{"cell_type":"markdown","metadata":{"id":"W0-dRCmmQTQs"},"source":["Hasta que no llamemos  ``loss.backward()`` los gradientes no se calculan.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657055833913,"user_tz":180,"elapsed":16,"user":{"displayName":"Luciano Robino","userId":"12024044421550105553"}},"outputId":"94a4397e-0a4e-4c57-d520-168b8c32997b","id":"VIswtgKuHXgs"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 0.0034,  0.0084,  0.0573, -0.0133, -0.0443, -0.0521, -0.0101, -0.0125,\n","        -0.0418,  0.0263], grad_fn=<SliceBackward0>)\n","None\n"]}],"source":["print(model[2].weight[0][0:10]) # solo algunos son mostrados\n","print(model[2].weight.grad)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RIppPw_QTQs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657055833914,"user_tz":180,"elapsed":15,"user":{"displayName":"Luciano Robino","userId":"12024044421550105553"}},"outputId":"f8dc8cff-60a4-4a53-ae29-d2dd72d80f99"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 0.0034,  0.0084,  0.0573, -0.0133, -0.0443, -0.0521, -0.0101, -0.0125,\n","        -0.0418,  0.0263], grad_fn=<SliceBackward0>)\n","tensor([ -8.1262,  -6.7792,  -3.4490, -11.4727,  -5.3070,  -1.5447,  -4.2072,\n","        -10.4046,  -5.4598,  -7.9765])\n"]}],"source":["loss.backward()\n","print(model[2].weight[0][0:10])\n","print(model[2].weight.grad[0][0:10])"]},{"cell_type":"markdown","metadata":{"id":"4vR5GYY1QTQs"},"source":["Por ahora solo hemos calculados los gradientes, pero no los hemos usada para actualizar los pesos. Esto es porque debemos ejecutar ``optimizer.step()``\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657055833914,"user_tz":180,"elapsed":13,"user":{"displayName":"Luciano Robino","userId":"12024044421550105553"}},"outputId":"030a88ea-b45d-49c5-91b4-bfceca89913f","id":"okTVR8gyHzzz"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 0.0034,  0.0084,  0.0573, -0.0133, -0.0443, -0.0521, -0.0101, -0.0125,\n","        -0.0418,  0.0263], grad_fn=<SliceBackward0>)\n","tensor([[-8.1262, -6.7792, -3.4490,  ..., -7.5278, -2.5329, -6.7232],\n","        [-3.7794,  2.7367,  3.4669,  ...,  3.2301, -1.3766,  2.9700],\n","        [-3.1742, -0.7591,  0.0477,  ..., -0.6937, -0.1774, -0.6788],\n","        ...,\n","        [ 2.3218, -0.5292, -2.4036,  ...,  0.9566,  0.3973,  0.1614],\n","        [-0.7314,  0.5804,  1.1974,  ..., -2.1463,  0.2211, -1.6682],\n","        [ 3.8156,  2.2354,  2.7665,  ...,  1.6837,  0.9417,  1.1036]])\n"]}],"source":["print(model[2].weight[0][0:10]) # solo algunos son mostrados\n","print(model[2].weight.grad[0][0:10])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_fdjOapqQTQt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657055834363,"user_tz":180,"elapsed":458,"user":{"displayName":"Luciano Robino","userId":"12024044421550105553"}},"outputId":"ee62a532-13a7-4210-fde2-7812abb58d7d"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 0.0115,  0.0152,  0.0607, -0.0018, -0.0390, -0.0506, -0.0059, -0.0021,\n","        -0.0363,  0.0343], grad_fn=<SliceBackward0>)\n","tensor([ -8.1262,  -6.7792,  -3.4490, -11.4727,  -5.3070,  -1.5447,  -4.2072,\n","        -10.4046,  -5.4598,  -7.9765])\n"]}],"source":["optimizer.step()\n","print(model[2].weight[0][0:10])\n","print(model[2].weight.grad[0][0:10])"]},{"cell_type":"markdown","metadata":{"id":"Squyv_l8QTQt"},"source":["Vemos ahora que los valores de ``model[2]`` han cambiado\n","\n","Un detalle que no dedemos ignorar es que debemos llamar a la función ``optimizer.zero_grad()`` despues de llamar\n","``optimizer.step()``. De no hacer esto cada vez que llamemos  ``loss.backward()`` la suma de los gradientes se acumulará.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z7Wcgm9OQTQv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657055834364,"user_tz":180,"elapsed":18,"user":{"displayName":"Luciano Robino","userId":"12024044421550105553"}},"outputId":"f3929d9d-f3e2-4942-c6bc-ac9e48610ef0"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ -8.1262,  -6.7792,  -3.4490, -11.4727,  -5.3070,  -1.5447,  -4.2072,\n","        -10.4046,  -5.4598,  -7.9765])\n","tensor([-22.1568, -17.8958, -10.1954, -16.0478, -14.2102,   4.8178,  -1.0685,\n","        -23.3787,  -2.2000, -26.5989])\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"]}],"source":["print(model[2].weight.grad[0][0:10])\n","\n","for i in range(0, 5):\n","    prediction = model(some_input)\n","    loss = (ideal_output - prediction).pow(2).sum()\n","    loss.backward()\n","    \n","print(model[2].weight.grad[0][0:10])\n","\n","optimizer.zero_grad()\n","\n","print(model[2].weight.grad[0][0:10])"]},{"cell_type":"markdown","metadata":{"id":"Hwps1UrKQTQ-"},"source":["Contenido adicional: Más información sobre Autograd\n","-----------------------------------------------------------\n","\n","En principio, ya conocíamos la noción de gradiente. Sabíamos que para una toma vectores m-dimensionales y devuelve un único valor (un escalar), $l=g\\left(\\vec{y}\\right)$ existe el gradiente. Esto es un vector que nos dice como varía una función conforme cambian los valores del vector de entrada $\\vec{y}$ \n","\n","\n","$$v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$$\n","\n","En general, si tenemos una función que toma vectores n-dimensionales como entrada y tiene como salida vectores m-dimensionales, $\\vec{y}=f(\\vec{x})$, la idea de gradiente no permite abarcar todas las posibles variaciones. En este sentido se necesita una generalización de la idea de gradiente. Esta generalización es una matriz conocida como el \n","*Jacobiano:*\n","\n","\\begin{align}J\n","     =\n","     \\left(\\begin{array}{ccc}\n","     \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n","     \\vdots & \\ddots & \\vdots\\\\\n","     \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n","     \\end{array}\\right)\\end{align}\n","\n","Sin embargo, la función de pérdida nuestros modelos más sencillos son en realidad una combinación de las dos cosas. \n","\n","$$l=g\\left(\\vec{y}\\right)$$\n","$$\\vec{y}=f(\\vec{x})$$\n","$$l=g\\left(f(\\vec{x})\\right)$$\n","\n","Puede demostrarse, sin embargo, que para obtener el gradiente de $l$, respecto de $\\vec{x}$ solo debemos hacer una multiplicación matricial\n","\n","$$\\vec{\\nabla_x} l=J^{T}\\cdot v$$\n","\n","\\begin{align}J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n","   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n","   \\vdots & \\ddots & \\vdots\\\\\n","   \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n","   \\end{array}\\right)\\left(\\begin{array}{c}\n","   \\frac{\\partial l}{\\partial y_{1}}\\\\\n","   \\vdots\\\\\n","   \\frac{\\partial l}{\\partial y_{m}}\n","   \\end{array}\\right)=\\left(\\begin{array}{c}\n","   \\frac{\\partial l}{\\partial x_{1}}\\\\\n","   \\vdots\\\\\n","   \\frac{\\partial l}{\\partial x_{n}}\n","   \\end{array}\\right)\\end{align}\n","\n","Del mismo modo, a la salida de cada capa, tenemos un Jacobiano distinto. De tal manera que nuestro gradiente en realdiad tendra la forma:\n","\n","$$\\vec{\\nabla_x} l=J_{1}^{T} J_{2}^{T} J_{3}^{T} J_{4}^{T}\\cdot v$$\n","\n","\n","**``torch.autograd`` es la herramienta que computa todas estas dependencias por medio de productos matriciales** Además de guardar la relación entre cada salida y cada entrada de cada capa\n","\n","\n"]},{"cell_type":"markdown","source":["Para más información consultar\n","\n","<https://pytorch.org/docs/stable/autograd.html#functional-higher-level-api>"],"metadata":{"id":"R_PjgZpSsewp"}}]}