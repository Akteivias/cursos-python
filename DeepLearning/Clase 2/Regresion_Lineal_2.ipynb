{"cells":[{"cell_type":"markdown","source":["# Regresión lineal concisa.\n","\n","En el notebook anterior, vimos un ejemplo de como implementar una red neuronal desde cero. Sin embargo, hacer esto es una mala idea. La principal razón por la que es una mala idea, es que muchas de las cosas que hicimos consisten en \"reinventar la rueda\". Hay bibliotecas que ya tienen herramientas para hacer lo que ya hicimos. Además, nuestra implementación puede no ser la más eficiente. Es decir: la implementación usada puede generar tiempos de espera que podrían ser evitados si nuestro código estuviera implementado de manera distinta. Por esta razón, es siempre recomendable usar las bibliotecas preexistentes.\n","\n","Recordemos que el ejemplo anterior estaba pensado para que le perdamos el miedo a las bibliotecas preexistentes, para que entendamos como funcionan y para aprender a implementar cosas nuevas (si llegamos a necesitarlo)\n","\n","Veamos entonces como implementariamos todo lo anterior haciendo uso de la biblioteca ``pytorch``"],"metadata":{"id":"AhE6f0rKIyFg"}},{"cell_type":"markdown","metadata":{"origin_pos":0,"id":"Ho7hYFsTIaPu"},"source":["\n","## Datos sintéticos\n"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":2,"tab":["pytorch"],"id":"zo70_UBVIaPw"},"outputs":[],"source":["import numpy as np\n","import torch\n","from torch.utils import data"]},{"cell_type":"markdown","source":["Misma función que usamos en el NB anterior"],"metadata":{"id":"kQX3OhaQ7wTf"}},{"cell_type":"code","source":["def synthetic_data(w, b, num_examples):\n","    X = torch.normal(0, 1, (num_examples, len(w)))\n","    y = torch.matmul(X, w) + b\n","    y += torch.normal(0, 0.01, y.shape)\n","    return X, y.reshape((-1, 1))"],"metadata":{"id":"Tmh5NYga01CX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":4,"tab":["pytorch"],"id":"s0adDvfCIaPy"},"outputs":[],"source":["true_w = torch.tensor([2, -3.4])\n","true_b = 4.2\n","features, labels = synthetic_data(true_w, true_b, 1000)"]},{"cell_type":"markdown","metadata":{"origin_pos":5,"id":"L89Foy1JIaPz"},"source":["## Cargando nuestros datos\n","\n","En este caso, podemos enviar nuestros datos diferentes metodos preexistentes de `pytorch` para generar nuestro minilotes.\n","\n","Además podemos pedir nos mezcle nuestros datos o que los deje tal cual\n"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":7,"tab":["pytorch"],"id":"_s7RVCfEIaP0"},"outputs":[],"source":["def load_array(data_arrays, batch_size, is_train=True):\n","    dataset = data.TensorDataset(*data_arrays)\n","    return data.DataLoader(dataset, batch_size, shuffle=is_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":9,"tab":["pytorch"],"id":"46yPkqcuIaP1"},"outputs":[],"source":["batch_size = 10\n","data_iter = load_array((features, labels), batch_size)"]},{"cell_type":"markdown","metadata":{"origin_pos":10,"id":"0QJFVBHMIaP2"},"source":["Queremos ver como se generan nuestros minilotes. Para esto debemos poder imprimierlos por pantalla. A diferencia de la implementación anterior, el método `DataLoader`, no genera un iterable, por esto debemos convertirlo en uno y recorrerlo segun necesitemos\n"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":11,"tab":["pytorch"],"id":"rJGz2ZwmIaP3","outputId":"239e86fd-e260-49f0-e4eb-e518003e6abe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656963758183,"user_tz":180,"elapsed":15,"user":{"displayName":"Luciano Robino","userId":"12024044421550105553"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[tensor([[-0.6694, -0.3256],\n","         [ 0.1160,  0.6049],\n","         [ 1.4336,  1.6383],\n","         [ 1.5785,  1.7636],\n","         [ 0.4943,  0.2662],\n","         [ 0.4037,  0.1017],\n","         [-1.1489,  0.8545],\n","         [-0.2937,  1.2940],\n","         [-0.2864, -0.0559],\n","         [-0.5227,  0.9123]]), tensor([[ 3.9669],\n","         [ 2.3659],\n","         [ 1.5006],\n","         [ 1.3614],\n","         [ 4.2735],\n","         [ 4.6633],\n","         [-0.9977],\n","         [-0.7902],\n","         [ 3.8183],\n","         [ 0.0464]])]"]},"metadata":{},"execution_count":6}],"source":["next(iter(data_iter))"]},{"cell_type":"markdown","metadata":{"origin_pos":12,"id":"c-MO_VboIaP3"},"source":["## Definiendo el modelo\n","\n","A continuación presentamos la versión concisa de nuestro modelo, luego la discutiremos.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":17,"tab":["pytorch"],"id":"xxpgLFRiIaP6"},"outputs":[],"source":["# `nn` = neural networks, redes neuronales\n","from torch import nn\n","\n","net = nn.Sequential(nn.Linear(2, 1))"]},{"cell_type":"markdown","source":["La clase `Sequiential` define todas las capas a aplicar de manera secuencial en nuestro modelo. Por ahora, como trabajamos con regresión lineal solo usaremos una capa. Sin embargo esta capa es lo que se llama una capa *totalmente conectada*. Es decir, esta representada por una matriz que aplica sobre vector de features. Al aplicar esta matriz encontramos la salida de nuestra neurona. En este caso, este tipo de capas se las conoce como `Linear` y reciben como entrada `(<numero_de_entradas>, <numero_de_salidas>)`. Para nuestro modelo, esto son nuestras 2 features y nuestra etiqueta."],"metadata":{"id":"KdAWZYUVMURe"}},{"cell_type":"markdown","source":["\n","¿Que es una capa con lineal?\n","------------------\n","\n","\n","una capa lineal o completamente conectada es la forma más básica de una red neuronal. Cada entrada influencia a cada salida de acuerdo a los pesos. Si nuestro modelo tiene $m$ entradas y $n$ salidas, la matriz de pesos sera $m \\times n$. De igual modo el vector de sesgos o bias tendra dimensión $n$"],"metadata":{"id":"YgBZAsPHq1fA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zqzGHdjVhhNJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656963758605,"user_tz":180,"elapsed":433,"user":{"displayName":"Luciano Robino","userId":"12024044421550105553"}},"outputId":"7d1c1769-f5fe-460d-d591-e2dcd532edd4"},"outputs":[{"output_type":"stream","name":"stdout","text":["entrada:\n","tensor([[0.7730, 0.8338]])\n","\n","\n","Pesos y parametros:\n","('weight', Parameter containing:\n","tensor([[ 0.0586, -0.0287]], requires_grad=True))\n","('bias', Parameter containing:\n","tensor([0.1773], requires_grad=True))\n","\n","\n","salida\n","tensor([[0.1987]], grad_fn=<AddmmBackward0>)\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[0.1987]], grad_fn=<AddBackward0>)"]},"metadata":{},"execution_count":8}],"source":["import torch\n","\n","lin = torch.nn.Linear(2, 1)\n","x = torch.rand(1, 2)\n","print('entrada:')\n","print(x)\n","\n","print('\\n\\nPesos y parametros:')\n","for param in lin.named_parameters():\n","    print(param)\n","\n","y = lin(x)\n","print('\\n\\nsalida')\n","print(y)\n","\n","# Al hacer la multiplicacion matricial correspondiente obtenemos nuesta salida.\n","x @ lin.weight.T + lin.bias"]},{"cell_type":"markdown","metadata":{"origin_pos":19,"id":"UDuEzQVaIaP6"},"source":["## Inicialización de parametros de nuestro modelo.\n","\n","Por lo general, los frameworks prexistentes tienen implementaciones por defecto para inicializar los parámetros. Sin embargo, queremos iniciarlos de manera similar a la anterior."]},{"cell_type":"markdown","metadata":{"origin_pos":21,"tab":["pytorch"],"id":"d4VMoRtWIaP7"},"source":["Para ellos accedemos a la primera (y única capa) usando `net[0]`. Luego accedemos a los pesos y los sesgos con `weight.data` and `bias.data`. Finalmente rellenamos los valores con lo que teníamos pensado usar.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":24,"tab":["pytorch"],"id":"04P7myKQIaP7","outputId":"9021a92d-a959-422a-b298-953e1fdf7092","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656963758606,"user_tz":180,"elapsed":21,"user":{"displayName":"Luciano Robino","userId":"12024044421550105553"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.])"]},"metadata":{},"execution_count":9}],"source":["net[0].weight.data.normal_(0, 0.01)\n","net[0].bias.data.fill_(0)"]},{"cell_type":"markdown","metadata":{"origin_pos":29,"id":"zLlMm_qEIaP-"},"source":["## Definiendo la función de pérdida\n"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":34,"tab":["pytorch"],"id":"f2zHtPgyIaP_"},"outputs":[],"source":["loss = nn.MSELoss()"]},{"cell_type":"markdown","metadata":{"origin_pos":36,"id":"pDAt2_m0IaP_"},"source":["## Definiendo el algoritmo de optimización\n"]},{"cell_type":"markdown","metadata":{"origin_pos":38,"tab":["pytorch"],"id":"7SImV0xVIaP_"},"source":["La principal diferencia con lo que hicimos antes, es solamente debemos pasarle a nuestro `SDG`, los parametros a optimizar. El resto de los detalles ya son manejados por la implementación de `pytorch`. En este caso también estamos pasando la tasa de aprendizaje, pero la clase `SGD` de `pytorch` ya incluye un valor por defecto."]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":41,"tab":["pytorch"],"id":"Q36F2-g2IaQA"},"outputs":[],"source":["trainer = torch.optim.SGD(net.parameters(), lr=0.03)"]},{"cell_type":"markdown","source":["Un optimizador en `torch` tiene por defecto una serie de métodos. Sin embargo ahora mismo solo nos interesan 2 de ellos, pues son los que más usaremos.\n","\n","* `Optimizer.step`\n","  > Este es el método es el que propiamente aplica el algoritmo SGD, o cualquier otro algoritmo que fueramos a implementar.\n","* `Optimizer.zero_grad`\n","  > Por defecto, `Optimizer` suma los sucesivos gradientes calculados. Esto hace que al principio de cada época de el entrenamiento, debamos setear el gradiente en 0. Es por esto que este método existe dentro de la clase `Optimizer`"],"metadata":{"id":"-HS9IpaWOqlt"}},{"cell_type":"markdown","metadata":{"origin_pos":43,"id":"Aph98AJ2IaQA"},"source":["## Entrenamiento\n","\n","Hasta aquí veníamos reduciendo lineas de código de manera impresionante. Sin embargo, nuestro ciclo de entrenamiento será casi identico a lo que habíamos visto antes.\n","* Repetimos hasta concluir\n","    * Calculamos la función de pérdida\n","    * Calculamos el gradiente con minilotes \n","    * Actualizamos los parámetros. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":45,"tab":["pytorch"],"id":"noX-wsqNIaQA","outputId":"7bee2c90-9a93-4484-da34-7256480dbeaa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656963759174,"user_tz":180,"elapsed":578,"user":{"displayName":"Luciano Robino","userId":"12024044421550105553"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 1, loss 0.000197\n","epoch 2, loss 0.000094\n","epoch 3, loss 0.000094\n"]}],"source":["num_epochs = 3\n","for epoch in range(num_epochs):\n","    for X, y in data_iter:\n","        l = loss(net(X) ,y)\n","        trainer.zero_grad()\n","        l.backward()\n","        trainer.step()\n","    l = loss(net(features), labels)\n","    print(f'epoch {epoch + 1}, loss {l:f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":49,"tab":["pytorch"],"id":"0ufuypfPIaQB","outputId":"cbc70d63-dab0-46ee-8477-af16d631b63d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656963759176,"user_tz":180,"elapsed":21,"user":{"displayName":"Luciano Robino","userId":"12024044421550105553"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["error in estimating w: tensor([0.0002, 0.0004])\n","error in estimating b: tensor([0.0003])\n"]}],"source":["w = net[0].weight.data\n","print('error in estimating w:', true_w - w.reshape(true_w.shape))\n","b = net[0].bias.data\n","print('error in estimating b:', true_b - b)"]},{"cell_type":"markdown","source":["Hasta aquí hemos trabajado con el problema de la regresión. Sin embargo, muchas veces lo que deseamos es clasificar segun clases discretas. De hecho, más adelante veremos que los grandes logros de las redes neuronales son en el area de clasificación. Para esto, a continuación hablaremos de Regresión Softmax y su aplicación en clasificación."],"metadata":{"id":"tDALUq9b_h2m"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"name":"Regresion_Lineal_2.ipynb","provenance":[{"file_id":"https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/chapter_linear-networks/linear-regression-concise.ipynb","timestamp":1656452654931}],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}