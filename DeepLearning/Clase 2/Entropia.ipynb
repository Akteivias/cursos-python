{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Entropia.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPMfhA8zXIEJ7e2EtyJafta"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Entropía e información\n","\n","Lo primero que haremos será señalar porque decimos que la entropía estadística es una noción asociada a la idea de información. Para esto pensaremos un juego para dos jugadores:\n","\n","* Materiales:\n","  * una bolsa o recipiente opaco\n","  * 4 pelotas con los números 1, 2, 3, 4\n","* Preparativos:\n","  * Se colocan las pelotas en la bolsa \n","  * El primer jugador saca una de las pelotas de la bolsa\n","* Objetivo general: Adivinar con el menor número de preguntas posibles cuál el número de la pelota que tiene el primer jugador .\n","  * Solo pueden hacerse preguntas que tengan como respuestas sí o no.\n","\n","No es dificil ver que este juego tiene la siguiente estrategia optima:\n","\n","```\n","1. Preguntar: \"¿El número es par?\"\n","  A. Si la respuesta es sí, preguntar: \"¿Es el número 4?\"\n","    a. Si la respuesta es sí, sabemos que es el número 4, hemos ganado.\n","    b. Si la respuesta es no, sabemos que es el número 2, hemos ganado.\n","  B. Si la respuesta es no, preguntar: \"¿Es el número 3?\"\n","    a. Si la respuesta es sí, sabemos que es el número 3, hemos ganado.\n","    b. Si la respuesta es no, sabemos que es el número 1, hemos ganado.\n","```\n","\n","En este juego, es fácil ver que está solución es optima porque solo hay una copia de cada pelota. Dicho de otro modo, nuestra estrategia podría cambiar si dentro de la bolsa, hubieran otras pelotas o si las distribución de las pelotas cambiara.\n","\n","El punto central de este ejercicio es que para este caso de 4 categorías **equiprobables** necesitamos 2 preguntas. \n","\n","Veamos esto:\n","\n","$$\\text{numero_promedio_de_preguntas} = \\sum_{i=1}^4 P(i)\\times\\text{numero_de_preguntas}(i)$$\n","\n","$$P(1) = \\dfrac{1}{4},~~\\text{numero_de_preguntas}(1) = 2$$\n","$$P(2) = \\dfrac{1}{4},~~\\text{numero_de_preguntas}(2) = 2$$\n","$$P(3) = \\dfrac{1}{4},~~\\text{numero_de_preguntas}(3) = 2$$\n","$$P(4) = \\dfrac{1}{4},~~\\text{numero_de_preguntas}(4) = 2$$\n","\n","$$\\text{numero_promedio_de_preguntas} = \\dfrac{1}{2} + \\dfrac{1}{2} + \\dfrac{1}{2} + \\dfrac{1}{2} = 2$$\n","\n","Cambiemos ahora nuestro juego de la siguiente manera:\n","\n","* Materiales:\n","  * 8 pelotas con los números 1, 1, 1, 1, 2, 2, 3, 4\n","\n","El hecho de que ahora cambiemos nuestra distribución de pelotas, hace que ahora la estrategia optima cambie:\n","\n","```\n","1. Preguntar: \"¿Es el número 1?\"\n","  A. Si la respuesta es sí, hemos ganado.\"\n","  B. Si la respuesta es no, preguntar: \"¿Es el número 2?\"\n","    a. Si la respuesta es sí, hemos ganado.\n","    b. Si la respuesta es no, preguntar: \"¿Es el número 3?.\n","      I. Si la respuesta es sí, sabemos que es el número 3, hemos ganado.\n","      I. Si la respuesta es no, sabemos que es el número 4, hemos ganado.\n","```\n","\n","calculemos ahora el número de preguntas, para ver que esta estragia es mejor\n","\n","$$\\text{numero_promedio_de_preguntas} = \\sum_{i=1}^4 P(i)\\times\\text{numero_de_preguntas}(i)$$\n","\n","$$P(1) = \\dfrac{4}{8} = \\dfrac{1}{2},~~\\text{numero_de_preguntas}(1) = 1$$\n","$$P(2) = \\dfrac{2}{8} = \\dfrac{1}{4},~~\\text{numero_de_preguntas}(2) = 2$$\n","$$P(3) = \\dfrac{1}{8},~~\\text{numero_de_preguntas}(3) = 3$$\n","$$P(4) = \\dfrac{1}{8},~~\\text{numero_de_preguntas}(4) = 3$$\n","\n","$$\\text{numero_promedio_de_preguntas} = \\dfrac{1}{2} + \\dfrac{1}{2} + \\dfrac{3}{8} +\\dfrac{3}{8} = 1.75$$\n","\n","El lector perpicaz debería notar las siguientes cosas:\n","\n","1. Lo que hemos hecho es exactamente lo mismo que uno hace cuando analiza algo con árboles de deciciones\n","2. El número de preguntas se corresponde la entropía estadística de la distribución\n","$$H = -\\sum_{i\\in \\text{clases}}P(i)\\ln P(i)$$\n","2. El resultado que hemos obtenido es similar a encontrar un código de Huffman para la pelotas\n","\n","Es esta relación entre número de preguntas y distribuciones lo que da lugar a la noción de entropía a la teoría de la información de Shannon\n","\n","Ahora, la siguiente pregunta que nos podemos hacer es que pasaría si uno de los jugadores del primer juego, tratara de jugar el primero. Es decir, ¿que pasa si usamos la estrategia optima del primer juego en el segundo juego? Lo que hay que ver es que en este paso usaremos el número de preguntas del primer juego con la probabilidad del segundo.\n","\n","$$P(1) = \\dfrac{4}{8} = \\dfrac{1}{2},~~\\text{numero_de_preguntas}(1) = 2$$\n","$$P(2) = \\dfrac{2}{8} = \\dfrac{1}{4},~~\\text{numero_de_preguntas}(2) = 2$$\n","$$P(3) = \\dfrac{1}{8},~~\\text{numero_de_preguntas}(3) = 2$$\n","$$P(4) = \\dfrac{1}{8},~~\\text{numero_de_preguntas}(4) = 2$$\n","\n","$$\\text{numero_promedio_de_preguntas} = 1 + \\dfrac{1}{2} + \\dfrac{1}{4} +\\dfrac{1}{4} = 2$$\n","\n","El número promedio de preguntas ha aumentado. \n","\n","Esto último puede parecer un mero ejercicio teórico, pero en realidad nos está diciendo mucho. En general cuando clasificamos, no conocemos las distribución real de las clases de nuestro problema. Del mismo modo, nuestro clasificador, implicitamente supone una distribución probabilistica (asociada a nuestra muestra). Es decir estamos en una situación donde creemos que estamos en un juego como el primero, pero tal vez en realidad es un juego como el segundo. Es en este sentido que tiene sentido calcular la entropia a partir de dos distribuciones distintas. La de nuestro clasificador y la de nuestros datos. Esto es lo que minimizamos con la entropía cruzada: Dado nuestro modelo, que tan bueno es reproduciendo los valore reales.\n","\n","$$CE(P,Q) = -\\sum_{i\\in \\text{clases}}P(i)\\ln Q(i)$$\n","\n","$Q$ es la distribución probabilsitica de nuestro modelo, $P$, la de nuestros datos"],"metadata":{"id":"MULs23SnG49d"}}]}