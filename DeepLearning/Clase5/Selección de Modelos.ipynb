{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Selección de Modelos.ipynb","provenance":[{"file_id":"1SF8QGmN9XXpbLTfRlJ8WEQlQrR1NQc-N","timestamp":1657295702711}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Selección de modelos\n","\n","En el aprendizaje automático, generalmente seleccionamos nuestro modelo final después de evaluar varios modelos candidatos. Este proceso se llama *selección de modelo*. A veces los modelos sujetos a comparación\n","son de naturaleza fundamentalmente diferente\n","(por ejemplo, árboles de decisión frente a modelos lineales). En otras ocasiones, estamos comparando miembros de la misma clase de modelos que han sido entrenados con diferentes configuraciones de hiperparámetros.\n","\n","Con los MLP, por ejemplo, es posible que deseemos comparar modelos con diferentes números de capas ocultas, diferentes números de unidades ocultas y varias opciones de funciones de activación aplicadas a cada capa oculta. Para determinar cuál es el mejor entre nuestros modelos candidatos, generalmente emplearemos un conjunto de datos de validación."],"metadata":{"id":"Ss--8RunX3Pw"}},{"cell_type":"markdown","source":["### Conjunto de datos de validación\n","\n","En principio, no deberíamos tocar nuestro conjunto de prueba hasta que hayamos elegido todos nuestros hiperparámetros.\n","Si utilizáramos los datos de prueba en el proceso de selección del modelo, existe el riesgo de que podamos sobreajustar los datos de prueba. Entonces estaríamos en serios problemas. Si sobreajustamos nuestros datos de entrenamiento, siempre existe la evaluación de los datos de prueba para mantenernos honestos. Pero si sobreajustamos los datos de prueba, ¿cómo lo sabríamos?\n","\n","Por lo tanto, nunca debemos confiar en los datos de prueba para la selección del modelo. Y, sin embargo, tampoco podemos confiar únicamente en los datos de entrenamiento para la selección del modelo porque no podemos estimar el error de generalización en los mismos datos que usamos para entrenar el modelo.\n","\n","\n","En aplicaciones prácticas, la imagen se vuelve más turbia. Si bien, idealmente, solo tocaríamos los datos de prueba una vez, para evaluar el mejor modelo o para comparar una pequeña cantidad de modelos entre sí, los datos de prueba del mundo real rara vez se descartan después de un solo uso. Rara vez podemos permitirnos un nuevo conjunto de prueba para cada ronda de experimentos.\n","\n","La práctica común para abordar este problema\n","es dividir nuestros datos de tres maneras, incorporando un *conjunto de datos de validación* (o *conjunto de validación*) además de los conjuntos de datos de entrenamiento y prueba. \n","\n","![Imgur](https://i.imgur.com/jyEPbG9.png)\n","\n","Un buen ejemplo para distinguir entre conjunto de prueba y de validación es lo que hace la plataforma Kaggle en sus competencias de aprendizaje automático. En sus inicios, Kaggle era solamente una plataforma de concursos donde las empresas publican problemas y los participantes compiten para construir el mejor algoritmo, generalmente con premios en efectivo. La organización d elos concursos consiste en:\n","1. el organizador debe separar su dataset en un conjunto de entrenamiento (que será publicado) y un conjunto de prueba (cuyas features serán publicadas, pero las etiquetas permanecerán ocultas). \n","2. Los participantes podrán descargar los datos de entrenamiento y deberán elegir un modelo para presentar en la competencia. Para eso, deberán llevar adelante una selección de modelos generando un conjunto de validación a partir de los datos de entrenamiento.\n","3. Una vez seleccionado el modelo que mejor funcione con los datos de validación, se alimenta dicho modelo con las features del conjunto de prueba para obtener las etiquetas de prueba predichas por el modelo.\n","4. Se entregan las etiquetas de prueba predichas y el organizador las compara con las reales. El ganador es el modelo que menos erroes haya cometido. \n","![Imgur](https://i.imgur.com/qA88YkJ.png)\n","\n","De esta manera, los conjuntos de prueba y validación están bien diferenciados. El primero se usa para elegir el mejor modelo y el segundo se usa para evaluar el modelo elegido con datos que nunca vio en el entrenamiento.\n","\n","A menos que se indique explícitamente lo contrario, en los experimentos de este curso en realidad estamos trabajando con lo que correctamente debería llamarse datos de entrenamiento y datos de validación, sin verdaderos conjuntos de prueba. Por lo tanto, reportado en cada experimento es realmente un accuracy de validación y no un verdadero accuracy del conjunto de pruebas."],"metadata":{"id":"S6jBugcwYTl1"}},{"cell_type":"markdown","source":["### $K$*-fold cross-validation*\n","\n","Cuando los datos de entrenamiento son escasos, es posible que ni siquiera podamos permitirnos mantener suficientes datos para constituir un conjunto de validación adecuado. Una solución popular a este problema es emplear $K$*-fold cross-validation*. Aquí, los datos de entrenamiento originales se dividen en $K$ subconjuntos que no se superponen. Luego, el entrenamiento y la validación del modelo se ejecutan $K$ veces, cada vez entrenando en $K-1$ subconjuntos y validando en un subconjunto diferente (el que no se usó para entrenar en esa ronda).\n","Finalmente, los errores de entrenamiento y validación se estiman promediando los resultados de los experimentos de $K$.\n","\n","![Imgur](https://i.imgur.com/SpOFGyK.png)"],"metadata":{"id":"Rx7BgvZndOmD"}},{"cell_type":"code","metadata":{"id":"sGt_fqSmOHgV","executionInfo":{"status":"ok","timestamp":1657295800795,"user_tz":180,"elapsed":4119,"user":{"displayName":"Pablo Marinozi","userId":"04218818404413865365"}}},"source":["import numpy as np\n","from sklearn.model_selection import KFold\n","\n","import torch\n","import torch.nn as nn\n","\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from torch.utils.data import DataLoader,ConcatDataset\n","\n","from torchvision import datasets, transforms\n","from torch.optim.lr_scheduler import StepLR\n","\n","from sklearn.model_selection import KFold"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ze-iF06lSzJU","executionInfo":{"status":"ok","timestamp":1657295806585,"user_tz":180,"elapsed":397,"user":{"displayName":"Pablo Marinozi","userId":"04218818404413865365"}}},"source":["class MNISTNet(nn.Module):\n","    def __init__(self):\n","        super(MNISTNet, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n","        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n","        self.dropout1 = nn.Dropout(0.25)\n","        self.dropout2 = nn.Dropout(0.5)\n","        self.fc1 = nn.Linear(9216, 128)\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = F.max_pool2d(x, 2)\n","        x = self.dropout1(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.dropout2(x)\n","        x = self.fc2(x)\n","        output = F.log_softmax(x, dim=1)\n","        return output"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"pHojQunc0R6R","executionInfo":{"status":"ok","timestamp":1657295809704,"user_tz":180,"elapsed":4,"user":{"displayName":"Pablo Marinozi","userId":"04218818404413865365"}}},"source":["def reset_weights(m):\n","    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n","        m.reset_parameters()"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"MoEEw_VpUY_R","executionInfo":{"status":"ok","timestamp":1657295817713,"user_tz":180,"elapsed":278,"user":{"displayName":"Pablo Marinozi","userId":"04218818404413865365"}}},"source":["def train(fold, model, device, train_loader, optimizer, epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.nll_loss(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % 500 == 0:\n","            print('Train Fold/Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                fold,epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"kGm1K0OfEiy_","executionInfo":{"status":"ok","timestamp":1657295830199,"user_tz":180,"elapsed":308,"user":{"displayName":"Pablo Marinozi","userId":"04218818404413865365"}}},"source":["def test(fold,model, device, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set for fold {}: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        fold,test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2UKHJeT0Ei1U","executionInfo":{"status":"ok","timestamp":1657295841051,"user_tz":180,"elapsed":325,"user":{"displayName":"Pablo Marinozi","userId":"04218818404413865365"}},"outputId":"abc3bce2-d90a-4e09-e152-92e874896a16"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","metadata":{"id":"J5uTGkQMIaKY","executionInfo":{"status":"ok","timestamp":1657295848076,"user_tz":180,"elapsed":443,"user":{"displayName":"Pablo Marinozi","userId":"04218818404413865365"}}},"source":["transform=transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,))\n","        ])"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"nB6dR_qwEi6x"},"source":["dataset1 = datasets.MNIST('../data', train=True, download=True,\n","                       transform=transform)\n","\n","dataset2 = datasets.MNIST('../data', train=False,\n","                       transform=transform)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ElLBng8Ei9f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657295883452,"user_tz":180,"elapsed":273,"user":{"displayName":"Pablo Marinozi","userId":"04218818404413865365"}},"outputId":"291882ee-237a-4a18-8f93-6f531d0974fd"},"source":["model = MNISTNet().to(device)\n","model.apply(reset_weights)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MNISTNet(\n","  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n","  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n","  (dropout1): Dropout(p=0.25, inplace=False)\n","  (dropout2): Dropout(p=0.5, inplace=False)\n","  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",")"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"A6Fkip8oEi4E","executionInfo":{"status":"ok","timestamp":1657295922977,"user_tz":180,"elapsed":329,"user":{"displayName":"Pablo Marinozi","userId":"04218818404413865365"}}},"source":["batch_size=32\n","folds=5\n","epochs=5\n","\n","kfold=KFold(n_splits=folds,shuffle=True)\n","\n","optimizer = optim.Adadelta(model.parameters())\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fr418n01EjAG","executionInfo":{"status":"ok","timestamp":1657295970764,"user_tz":180,"elapsed":276,"user":{"displayName":"Pablo Marinozi","userId":"04218818404413865365"}}},"source":["dataset=ConcatDataset([dataset1,dataset2])"],"execution_count":14,"outputs":[]},{"cell_type":"code","source":["for train_idx,test_idx in kfold.split(dataset):\n","  print(test_idx)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b14TnOiviPGP","executionInfo":{"status":"ok","timestamp":1657296244507,"user_tz":180,"elapsed":442,"user":{"displayName":"Pablo Marinozi","userId":"04218818404413865365"}},"outputId":"811f87cd-9bec-4711-e5fa-fc8a14e6671d"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["[    1     9    10 ... 69991 69993 69996]\n","[    2     4     8 ... 69977 69990 69997]\n","[   11    24    26 ... 69989 69992 69994]\n","[    5     6    13 ... 69983 69986 69998]\n","[    0     3     7 ... 69984 69995 69999]\n"]}]},{"cell_type":"code","metadata":{"id":"6eX4vxj8EjC1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657296768219,"user_tz":180,"elapsed":411286,"user":{"displayName":"Pablo Marinozi","userId":"04218818404413865365"}},"outputId":"4ad5ac67-3b79-48af-cd4d-1561a6a645de"},"source":["for fold,(train_idx,test_idx) in enumerate(kfold.split(dataset)):\n","  print('------------fold no---------{}----------------------'.format(fold))\n","  train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n","  test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n","\n","  trainloader = torch.utils.data.DataLoader(\n","                      dataset, \n","                      batch_size=batch_size, sampler=train_subsampler)\n","  testloader = torch.utils.data.DataLoader(\n","                      dataset,\n","                      batch_size=batch_size, sampler=test_subsampler)\n","\n","  model.apply(reset_weights)\n","\n","  for epoch in range(1, epochs + 1):\n","    train(fold, model, device, trainloader, optimizer, epoch)\n","    test(fold,model, device, testloader)"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["------------fold no---------0----------------------\n","Train Fold/Epoch: 0/1 [0/70000 (0%)]\tLoss: 2.329220\n","Train Fold/Epoch: 0/1 [16000/70000 (29%)]\tLoss: 0.069491\n","Train Fold/Epoch: 0/1 [32000/70000 (57%)]\tLoss: 0.174078\n","Train Fold/Epoch: 0/1 [48000/70000 (86%)]\tLoss: 0.206182\n","\n","Test set for fold 0: Average loss: 0.0132, Accuracy: 13720/70000 (20%)\n","\n","Train Fold/Epoch: 0/2 [0/70000 (0%)]\tLoss: 0.006167\n","Train Fold/Epoch: 0/2 [16000/70000 (29%)]\tLoss: 0.008713\n","Train Fold/Epoch: 0/2 [32000/70000 (57%)]\tLoss: 0.263678\n","Train Fold/Epoch: 0/2 [48000/70000 (86%)]\tLoss: 0.008075\n","\n","Test set for fold 0: Average loss: 0.0105, Accuracy: 13785/70000 (20%)\n","\n","Train Fold/Epoch: 0/3 [0/70000 (0%)]\tLoss: 0.002479\n","Train Fold/Epoch: 0/3 [16000/70000 (29%)]\tLoss: 0.139068\n","Train Fold/Epoch: 0/3 [32000/70000 (57%)]\tLoss: 0.025168\n","Train Fold/Epoch: 0/3 [48000/70000 (86%)]\tLoss: 0.079616\n","\n","Test set for fold 0: Average loss: 0.0106, Accuracy: 13807/70000 (20%)\n","\n","Train Fold/Epoch: 0/4 [0/70000 (0%)]\tLoss: 0.145906\n","Train Fold/Epoch: 0/4 [16000/70000 (29%)]\tLoss: 0.013685\n","Train Fold/Epoch: 0/4 [32000/70000 (57%)]\tLoss: 0.207472\n","Train Fold/Epoch: 0/4 [48000/70000 (86%)]\tLoss: 0.223118\n","\n","Test set for fold 0: Average loss: 0.0088, Accuracy: 13811/70000 (20%)\n","\n","Train Fold/Epoch: 0/5 [0/70000 (0%)]\tLoss: 0.022071\n","Train Fold/Epoch: 0/5 [16000/70000 (29%)]\tLoss: 0.014207\n","Train Fold/Epoch: 0/5 [32000/70000 (57%)]\tLoss: 0.060805\n","Train Fold/Epoch: 0/5 [48000/70000 (86%)]\tLoss: 0.020781\n","\n","Test set for fold 0: Average loss: 0.0090, Accuracy: 13817/70000 (20%)\n","\n","------------fold no---------1----------------------\n","Train Fold/Epoch: 1/1 [0/70000 (0%)]\tLoss: 2.330793\n","Train Fold/Epoch: 1/1 [16000/70000 (29%)]\tLoss: 0.066010\n","Train Fold/Epoch: 1/1 [32000/70000 (57%)]\tLoss: 0.002944\n","Train Fold/Epoch: 1/1 [48000/70000 (86%)]\tLoss: 0.008098\n","\n","Test set for fold 1: Average loss: 0.0109, Accuracy: 13772/70000 (20%)\n","\n","Train Fold/Epoch: 1/2 [0/70000 (0%)]\tLoss: 0.057661\n","Train Fold/Epoch: 1/2 [16000/70000 (29%)]\tLoss: 0.033335\n","Train Fold/Epoch: 1/2 [32000/70000 (57%)]\tLoss: 0.033286\n","Train Fold/Epoch: 1/2 [48000/70000 (86%)]\tLoss: 0.031949\n","\n","Test set for fold 1: Average loss: 0.0090, Accuracy: 13795/70000 (20%)\n","\n","Train Fold/Epoch: 1/3 [0/70000 (0%)]\tLoss: 0.144018\n","Train Fold/Epoch: 1/3 [16000/70000 (29%)]\tLoss: 0.002379\n","Train Fold/Epoch: 1/3 [32000/70000 (57%)]\tLoss: 0.008066\n","Train Fold/Epoch: 1/3 [48000/70000 (86%)]\tLoss: 0.005517\n","\n","Test set for fold 1: Average loss: 0.0090, Accuracy: 13813/70000 (20%)\n","\n","Train Fold/Epoch: 1/4 [0/70000 (0%)]\tLoss: 0.002517\n","Train Fold/Epoch: 1/4 [16000/70000 (29%)]\tLoss: 0.011590\n","Train Fold/Epoch: 1/4 [32000/70000 (57%)]\tLoss: 0.014429\n","Train Fold/Epoch: 1/4 [48000/70000 (86%)]\tLoss: 0.001438\n","\n","Test set for fold 1: Average loss: 0.0092, Accuracy: 13834/70000 (20%)\n","\n","Train Fold/Epoch: 1/5 [0/70000 (0%)]\tLoss: 0.010561\n","Train Fold/Epoch: 1/5 [16000/70000 (29%)]\tLoss: 0.000147\n","Train Fold/Epoch: 1/5 [32000/70000 (57%)]\tLoss: 0.006590\n","Train Fold/Epoch: 1/5 [48000/70000 (86%)]\tLoss: 0.001950\n","\n","Test set for fold 1: Average loss: 0.0095, Accuracy: 13803/70000 (20%)\n","\n","------------fold no---------2----------------------\n","Train Fold/Epoch: 2/1 [0/70000 (0%)]\tLoss: 2.314187\n","Train Fold/Epoch: 2/1 [16000/70000 (29%)]\tLoss: 0.475445\n","Train Fold/Epoch: 2/1 [32000/70000 (57%)]\tLoss: 0.253448\n","Train Fold/Epoch: 2/1 [48000/70000 (86%)]\tLoss: 0.074320\n","\n","Test set for fold 2: Average loss: 0.0126, Accuracy: 13750/70000 (20%)\n","\n","Train Fold/Epoch: 2/2 [0/70000 (0%)]\tLoss: 0.573862\n","Train Fold/Epoch: 2/2 [16000/70000 (29%)]\tLoss: 0.123097\n","Train Fold/Epoch: 2/2 [32000/70000 (57%)]\tLoss: 0.002058\n","Train Fold/Epoch: 2/2 [48000/70000 (86%)]\tLoss: 0.012375\n","\n","Test set for fold 2: Average loss: 0.0100, Accuracy: 13798/70000 (20%)\n","\n","Train Fold/Epoch: 2/3 [0/70000 (0%)]\tLoss: 0.114996\n","Train Fold/Epoch: 2/3 [16000/70000 (29%)]\tLoss: 0.007008\n","Train Fold/Epoch: 2/3 [32000/70000 (57%)]\tLoss: 0.544099\n","Train Fold/Epoch: 2/3 [48000/70000 (86%)]\tLoss: 0.002690\n","\n","Test set for fold 2: Average loss: 0.0093, Accuracy: 13804/70000 (20%)\n","\n","Train Fold/Epoch: 2/4 [0/70000 (0%)]\tLoss: 0.014245\n","Train Fold/Epoch: 2/4 [16000/70000 (29%)]\tLoss: 0.004196\n","Train Fold/Epoch: 2/4 [32000/70000 (57%)]\tLoss: 0.041229\n","Train Fold/Epoch: 2/4 [48000/70000 (86%)]\tLoss: 0.007031\n","\n","Test set for fold 2: Average loss: 0.0091, Accuracy: 13806/70000 (20%)\n","\n","Train Fold/Epoch: 2/5 [0/70000 (0%)]\tLoss: 0.168874\n","Train Fold/Epoch: 2/5 [16000/70000 (29%)]\tLoss: 0.017900\n","Train Fold/Epoch: 2/5 [32000/70000 (57%)]\tLoss: 0.037826\n","Train Fold/Epoch: 2/5 [48000/70000 (86%)]\tLoss: 0.034665\n","\n","Test set for fold 2: Average loss: 0.0097, Accuracy: 13807/70000 (20%)\n","\n","------------fold no---------3----------------------\n","Train Fold/Epoch: 3/1 [0/70000 (0%)]\tLoss: 2.279423\n","Train Fold/Epoch: 3/1 [16000/70000 (29%)]\tLoss: 0.072851\n","Train Fold/Epoch: 3/1 [32000/70000 (57%)]\tLoss: 0.252965\n","Train Fold/Epoch: 3/1 [48000/70000 (86%)]\tLoss: 0.015013\n","\n","Test set for fold 3: Average loss: 0.0122, Accuracy: 13767/70000 (20%)\n","\n","Train Fold/Epoch: 3/2 [0/70000 (0%)]\tLoss: 0.008568\n","Train Fold/Epoch: 3/2 [16000/70000 (29%)]\tLoss: 0.003360\n","Train Fold/Epoch: 3/2 [32000/70000 (57%)]\tLoss: 0.003677\n","Train Fold/Epoch: 3/2 [48000/70000 (86%)]\tLoss: 0.029382\n","\n","Test set for fold 3: Average loss: 0.0116, Accuracy: 13781/70000 (20%)\n","\n","Train Fold/Epoch: 3/3 [0/70000 (0%)]\tLoss: 0.007695\n","Train Fold/Epoch: 3/3 [16000/70000 (29%)]\tLoss: 0.019118\n","Train Fold/Epoch: 3/3 [32000/70000 (57%)]\tLoss: 0.160773\n","Train Fold/Epoch: 3/3 [48000/70000 (86%)]\tLoss: 0.031795\n","\n","Test set for fold 3: Average loss: 0.0112, Accuracy: 13807/70000 (20%)\n","\n","Train Fold/Epoch: 3/4 [0/70000 (0%)]\tLoss: 0.006151\n","Train Fold/Epoch: 3/4 [16000/70000 (29%)]\tLoss: 0.008217\n","Train Fold/Epoch: 3/4 [32000/70000 (57%)]\tLoss: 0.009990\n","Train Fold/Epoch: 3/4 [48000/70000 (86%)]\tLoss: 0.001056\n","\n","Test set for fold 3: Average loss: 0.0100, Accuracy: 13839/70000 (20%)\n","\n","Train Fold/Epoch: 3/5 [0/70000 (0%)]\tLoss: 0.002625\n","Train Fold/Epoch: 3/5 [16000/70000 (29%)]\tLoss: 0.001393\n","Train Fold/Epoch: 3/5 [32000/70000 (57%)]\tLoss: 0.000880\n","Train Fold/Epoch: 3/5 [48000/70000 (86%)]\tLoss: 0.015396\n","\n","Test set for fold 3: Average loss: 0.0095, Accuracy: 13832/70000 (20%)\n","\n","------------fold no---------4----------------------\n","Train Fold/Epoch: 4/1 [0/70000 (0%)]\tLoss: 2.308650\n","Train Fold/Epoch: 4/1 [16000/70000 (29%)]\tLoss: 0.140487\n","Train Fold/Epoch: 4/1 [32000/70000 (57%)]\tLoss: 0.163790\n","Train Fold/Epoch: 4/1 [48000/70000 (86%)]\tLoss: 0.046051\n","\n","Test set for fold 4: Average loss: 0.0103, Accuracy: 13791/70000 (20%)\n","\n","Train Fold/Epoch: 4/2 [0/70000 (0%)]\tLoss: 0.013688\n","Train Fold/Epoch: 4/2 [16000/70000 (29%)]\tLoss: 0.153544\n","Train Fold/Epoch: 4/2 [32000/70000 (57%)]\tLoss: 0.016689\n","Train Fold/Epoch: 4/2 [48000/70000 (86%)]\tLoss: 0.121687\n","\n","Test set for fold 4: Average loss: 0.0088, Accuracy: 13806/70000 (20%)\n","\n","Train Fold/Epoch: 4/3 [0/70000 (0%)]\tLoss: 0.053579\n","Train Fold/Epoch: 4/3 [16000/70000 (29%)]\tLoss: 0.010856\n","Train Fold/Epoch: 4/3 [32000/70000 (57%)]\tLoss: 0.245438\n","Train Fold/Epoch: 4/3 [48000/70000 (86%)]\tLoss: 0.063053\n","\n","Test set for fold 4: Average loss: 0.0077, Accuracy: 13833/70000 (20%)\n","\n","Train Fold/Epoch: 4/4 [0/70000 (0%)]\tLoss: 0.066106\n","Train Fold/Epoch: 4/4 [16000/70000 (29%)]\tLoss: 0.002654\n","Train Fold/Epoch: 4/4 [32000/70000 (57%)]\tLoss: 0.012990\n","Train Fold/Epoch: 4/4 [48000/70000 (86%)]\tLoss: 0.073346\n","\n","Test set for fold 4: Average loss: 0.0086, Accuracy: 13843/70000 (20%)\n","\n","Train Fold/Epoch: 4/5 [0/70000 (0%)]\tLoss: 0.002172\n","Train Fold/Epoch: 4/5 [16000/70000 (29%)]\tLoss: 0.000217\n","Train Fold/Epoch: 4/5 [32000/70000 (57%)]\tLoss: 0.024289\n","Train Fold/Epoch: 4/5 [48000/70000 (86%)]\tLoss: 0.002732\n","\n","Test set for fold 4: Average loss: 0.0084, Accuracy: 13850/70000 (20%)\n","\n"]}]},{"cell_type":"code","metadata":{"id":"ertpL4r1EjGX"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u_CF8xJwEjIo"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-oUQwmbtEjLt"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"deyLmTH1EjOi"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TVOVW7z_EjRa"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fOqDl0utEjUM"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xqjYoVOrEjXT"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QHuEtlUoEjZ4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WkhK4sSwEjcy"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vl4J9rerEjfd"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QmMihYR-EjiN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ewuTFB7JEjkl"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T_YvRnTZEjnt"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1R5KDRwmUZPO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xcBeVQ0hUZdP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OVeeTn-6UZqq"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P09PpwkjUZ4u"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6cAt2uNHUaT6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gw--2z3LUaiy"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"un41wu-1Ua-d"},"source":[""],"execution_count":null,"outputs":[]}]}